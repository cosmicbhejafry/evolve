Darwin's theory of evolution through natural selection has been central to the
study of evolution. The modern theory of genetics  -- phenotype -- genotype --
regulatory networks. 

Valiant (2006) introduced a model of evolution to address the question of
complexity that may arise through Darwinian mechanisms. In particular, in
Valiant's model a creature is a representation that computes a boolean function
on some domain $X$. The goal is to compute a function that is close to an
\emph{ideal function} with respect to some underlying distribution on $X$.
Valiant defines what feasible mutations may be applied to a representation, and
how (natural) selection may be modeled. The model is described in some detail in
Section~\ref{sec:evolve-model}, but the main idea is that from the pool of
possible mutations, one that increases performance is selected. In the spirit of
computational learning theory, the question is, for what classes of ideal
functions does evolution always succeed within feasible resources?

In a remarkable result, Feldman~\cite{Feldman:2008}, showed that the class of
functions \emph{evolvable} in Valiant's model is exactly captured by the class
of functions learnable in a restriction of Kearns' statistical query
model~\cite{Kearns:1998}, the correlational SQ model. Feldman's reduction
cleverly encodes the state of the algorithm, including the queries it may make,
and shows that combined with selection of beneficial mutations this essentially
simulates query responses. 

P. Valiant considers the evolution of \emph{real-valued}
functions~\cite{Valiant:2012-reals}. When considering real-valued functions, the
choice of \emph{loss} or \emph{performance} metric that measures the distance
between the candidate and ideal function becomes important. P. Valiant showed
that,  -- weak oracle -- optimization -- similar to Feldman's reduction.

On the other hand, more direct evolutionary algorithms have been considered.
Valiant, in his original paper, showed that under the uniform distribution the
class of disjunctions are evolvable by a set of simple
mutations~\cite{Valiant:2009}. Later, Kanade, Valiant and Vaughan showed that
the class of homogeneous linear separators under radially symmetric
distributions may also be evolved through simple mutations~\cite{KVV:2010}. P.
Valiant showed that under squared loss, the class of low-degree polynomials is
evolvable, again using essentially random mutations.

\subsection{Representation in Biology}

In Valiant's model, a creature is regarded as computing a boolean function from
some domain $X$, $r : X \rightarrow \{0, 1\}$. Valiant allows the representation
of $\langle r \rangle$ to be an arbitrary binary string, such that there is an
\emph{efficient} Turing machine $M$ that, given the description $\langle r
\rangle$ and $x \in X$ as input, outputs $r(x)$. The generality of this
definition is appealing because it very elegantly captures the outer limits of
evolution, under the Church-Turing hypothesis. However, this generality also has
the effect of allowing the encoding of arbitrary computations, this is
inherent in the proof of equivalence between evolvability and correlational
statistical query learning.

Real biological systems somehow function with conspicuously restricted
representation classes. Of particular interest to us here is that sparsity is a
ubiquitous property in networks of interacting genes and proteins that respond
to the environment and regulate cellular function.
Biological systems can be constrained in ways that correspond to other suitable
restrictions on the class of representations -- for example, cite Paul and give
an argument for bounded quantities (discrete, finite numbers of molecules).

We focus on transcription networks, which are a specific class of
networks of interacting genes and proteins. For a splendid introduction to
transcription networks and other biological circuits, see Alon~\cite{alon};
below we review some motivating and simplified concepts.
A protein is encoded in DNA as a gene, that when transcribed produces mRNA
that is translated into protein (Figure~\ref{}).

\todoea{Figure: transcription and a simple network.}

In a transcription network, a gene's expression, which can be
thought of as the rate at which resultant the protein is produced,
may be regulated by a set of proteins called \emph{transcription factors} (TFs).
The protein encoded by a gene may itself be a TF that regulates other genes,
or some other ``output'' of interest, such as an enzyme that catalyzes a
metabolic reaction.
In this discussion, we focus on feed-forward transcription networks without
loops, i.e., networks that can be represented as directed acyclic graphs.
In general, transcription networks can have cycles, e.g., a feed-forward loop
with output that inhibits or further activates an upstream component.

\todoea{100's of TFs and 1000's of genes.}

TFs represent information about the environment,
for example by changing shape when bound to by a certain small molecule.
A TF regulates a gene's expression by binding to a region of DNA close to the
gene; the shape of a TF affects its ability to bind,
which in turn can have a positive or negative effect on gene production.
The binding of TFs to DNA introduces physical limits on how many TFs may affect
a particular gene's expression.

\todoea{Most genes have L-U binding sites.}

An important property of a transcription network is its depth, i.e.,
the number of steps before a desired output is produced. In transcription
networks, in order for protein production to keep up with cell division,
it must occur on a time scale of the same order of magnitude. Hence, the
number of steps, before the desired output is produced is typically very small.

\todoea{Numbers regarding depth.}

Thus, a natural restriction one may place on the representation used by
evolution is that it must be the description of a constant-depth circuit with
constant fan-in gates. Of course, any such function can depend only on a
constant number of all possible variables. Thus, we consider the class of
functions restricted so that each function depends only on a constant number of
variables.

%Other important constraints on transcription networks arise from the fact that
%a dividing cell must replicate its DNA and essentially double its protein
%levels, including the replication machinery...

\section{Our Contributions}

Motivated by properties of biological systems, we consider evolvability of
sparse linear functions. ... blah ..

blah ..
