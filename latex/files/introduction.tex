Darwin's theory of evolution through natural selection has been central to the
study of evolution. The modern theory of genetics  -- phenotype -- genotype --
regulatory networks. 

Valiant (2006) introduced a model of evolution to address the question of
complexity that may arise through Darwinian mechanisms. In this model, an
organism is an entity that computes a function of its environment. There is a
hypothetical \emph{ideal function} indicating the best behaviour in every
possible environment. The performance of the organism is measured by how close
the function it computes is to the ideal. Mutations may alter the function an
organism computes. The performance (fitness) forms the basis of natural
selection. The resources allowed are the most generous while remaining feasible,
the mutation mechanism may be any efficient randomised Turing machine, and the
function represented by the organism may be arbitrary as long as it may be
computable by an efficient Turing machine.

Formulated this way, the question of evolvability can be asked in the language
of computational learning theory. For what classes of functions, $C$, can one
expect to find an evolutionary mechanism that gets arbitrarily close to the
ideal, within feasible computational resources? Darwinian selection is
restrictive in the sense that the only feedback received is aggregate over life
experiences. Valiant observed that any feasible evolutionary mechanism could be
simulated in the statistical query framework of Kearns~\cite{Kearns:1994}. In a
remarkable result, Feldman showed that in fact evolvable concept classes are
exactly captured by a restriction of Kearns's model, where the learning algorithm
is only allowed to make \emph{performance queries}, \ie, it produces a hypothesis
and then makes a query that returns the (approximate) loss of that hypothesis under the
distribution.\footnote{Feldman calls these queries correlational statistical
queries, because when working with Boolean functions with range $\{-1, 1\}$, the
performance of any hypothesis is its correlation with the ideal function.} P.
Valiant studied the evolvability of real-valued functions. He showed that
whenever the corresponding weak optimisation problem, \ie, approximately
minimising the expected loss, can be solved by a using a weak-evaluation oracle,
such an algorithm can be converted into an evolutionary
mechanism~\cite{Valiant:2012-real}. This implies that a large of class of
functions -- low degree real polynomials -- can be evolved with respect to any
convex loss function.

Direct evolutionary mechanisms \dots \eanote{Summarize what these are and
what they achieve -- I don't totally understand what ``direct'' means.}
Valiant~\cite{Valiant:2009-evolvability} showed that the class of
disjunctions is evolvable using a simple set of mutations under the uniform
distribution. Kanade, Valiant and Vaughan proposed a simple mechanism for
evolving homogeneous linear separators under radially symmetric
distributions~\cite{KVV:2010-drift}.  Feldman considered a model where the
ideal function is boolean but the representation is real-valued, allowing for
more detailed feedback. He presents an algorithm for evolving large margin
linear separators for a large class of loss functions~\cite{Feldman:2011-LTF}.
P.  Valiant also showed that with very simple mutations, the class of low-degree
polynomials can be evolved with respect to the squared loss~\cite{Valiant:2012-real}.

\todovk{I'd like this paragraph re-worded.}
\todoea{I think much of this paragraph could be deleted.}
Modeling mutations as the outputs of a Turing machine offers appealing
generality that captures a fundamental notion of the evolvable that is not
specific to life on Earth, \eg, requiring that creatures use sequences of DNA
to represent their hypotheses.
At the same time, the more direct algorithms are appealing because \dots
\eanote{Because of their ``naturalness''?}
However, it is difficult to define what constraints ``naturalness'' might place
on mutations.  Our current understanding of biological mutations and the
relationship between genotype -- the representation of a hypothesis --
and phenotype -- its corresponding functional output -- is incredibly limited.
Thus, Valiant~\cite{Valiant:2013-PAC} argued that a temptation to define
\emph{naturalness} should be resisted, \eanote{while observing that natural
systems are constrained?}

In this work, we suggest that the other general aspect in Valiant's model, 
\eanote{What is the first general aspect?} representations being arbitrary
polynomial-time computable functions, may indeed benefit from a closer study of
biology. We first describe a particular class of biological circuits,
\emph{transcription networks}, that motivate our study. We then frame the
question in the language of computational learning theory, summarize our
contributions and discuss related work.

\subsection{Representation in Biology}

\todovk{I don't like this paragraph.. I don't know what to do.}
Biological systems appear to function successfully with greatly restricted
representation classes. For example, \emph{Escherichia coli} is a single-celled
bacterium that has thrived for millions of years in the digestive systems of
animal hosts; its responses to the environment -- which is dynamic and very
high-dimensional -- are encoded in about 4,000 genes~\cite{biology}. Notably,
sparsity is a ubiquitous property in networks of interacting genes and proteins
that respond to the environment and regulate cellular function.  Biological
systems can be constrained in ways that correspond to other suitable
restrictions on the class of representations -- for example, cite Paul and give
an argument for bounded quantities (discrete, finite numbers of molecules).

We focus on transcription networks, which are a specific class of networks of
interacting genes and proteins. For an accessible and mathematical introduction to
transcription networks and other biological circuits, see (Alon~\cite{alon});
below we present a simplified account that motivates this work. A protein is
encoded in DNA as a gene, that when transcribed produces mRNA that is translated
into protein (Figure~\ref{}).

\todoea{Figure: transcription and a simple network.}

In a transcription network, a gene's expression, which can be thought of as the
\vknote{quantity of (rate at which)} resultant the protein is produced, may be
regulated by a set of proteins called \emph{transcription factors} (TFs). Some
TFs are always present in the cell and represent a \emph{snapshot} of the
environment. For example, when sugar molecules are present in the environment,
some transcription factors may get activated. These activated transcription
factors, may regulate the production of another protein. Sometimes, the produced
protein may itself be a transcription factor (which we view as intermediate
computation); and at other times a desired output, such as a catalytic enzyme.
While, biological systems indeed to include feed-back loop, here for simplicity
we focus on systems that are directed acyclic graphs, and the resulting
computation can be viewed as a circuit.

\vknote{ I added some of these to the prvious paragraph.
The protein encoded by a gene may itself be a TF that regulates other genes, or
some other ``output'' of interest, such as an enzyme that catalyzes a metabolic
reaction.  In this discussion, we focus on feed-forward transcription networks
without loops, i.e., networks that can be represented as directed acyclic
graphs.  In general, transcription networks can have cycles, e.g., a
feed-forward loop with output that inhibits or further activates an upstream
component.

TFs represent information about the environment,
for example by changing shape when bound to by a certain small molecule.
A TF regulates a gene's expression by binding to a region of DNA close to the
gene; the shape of a TF affects its ability to bind,
which in turn can have a positive or negative effect on gene production.
The binding of TFs to DNA introduces physical limits on how many TFs may affect
a particular gene's expression.
}

\todoea{Most genes have L-U binding sites.}
The number of transcription factors may very from hundreds in a bacteirum to
thousands in a human cell. The way transcription factors affect production of a
protein is by physically binding to a region of the DNA, close to where the
protein description is encoded. Thus, physical limitations demand that only a
small number of TFs may regulate a single gene. A second feature, of
transcription networks is that the time required for sufficient quantities of
protein to be produced is of the same magnitude as cell-division time. Thus,
these circuits are by necessity shallow.\footnote{Other kinds of networks, such
as signalling networks, work by changing shapes of proteins. The fact that these
transformations are rapid may allow for much larger depth.}
\todoea{Numbers regarding depth.}
\todovk{I don't understand your other two biological notes.}


\vknote{I incoroporated most of these things. So somethings can be safely
deleted in the next iteration.
An important property of a transcription network is its depth, i.e.,
the number of steps before a desired output is produced. In transcription
networks, in order for protein production to keep up with cell division,
it must occur on a time scale of the same order of magnitude. Hence, the
number of steps, before the desired output is produced is typically very small.


Thus, a natural restriction one may place on the representation used by
evolution is that it must be the description of a constant-depth circuit with
constant fan-in gates. Of course, any such function can depend only on a
constant number of all possible variables. Thus, we consider the class of
functions restricted so that each function depends only on a constant number of
variables.
}

%Other important constraints on transcription networks arise from the fact that
%a dividing cell must replicate its DNA and essentially double its protein
%levels, including the replication machinery...

\subsection{Our Contributions}

First, our contribution is conceptual. We believe the study of evolvability from
a computational standpoint will benefit by understanding the representation
complexity of the organism. Motivated by the previous discussion, in the case of
transcription networks, one may insist that the representation be a
constant depth and fan-in (boolean or arithmetic) circuit. Of course, any
function that can be represented by such a circuit can depend only on a constant
number of input variables.

Second, we show that the class of sparse linear functions, those that depend
only on a constant number of variables, can be evolved using sparse linear
functions, under a large class of smooth distributions, when the performance is
measured using squared error. The number of variables used by the
representations is larger than the number of variables in the \emph{ideal
function} and depends on the \emph{smoothness} parameter of the distribution.
Note that a linear function is represented by a weighted arithmetic circuit with
only one addition gate (alternatively a depth two circuit, with a layer of
multiplication gates and some constant inputs). There is also a natural
trade-off between depth and fan-in. For the precise statement, see
Theorem~\ref{thm:sparse-linear} in Section~\ref{sec:sparse-linear}.

Valiant also proposed a stronger selection mechanism, when natural selection
aggressively selects the (almost) best mutation, than merely a beneficial one --
called evolution by optimisation.  Under stronger conditions on the
distribution, we show that under evolution by optimisation sparse linear
functions can be evolved by representations with the same sparsity. This
algorithm however requires to be initialised, \ie the evolutionary process
begins with the $0$ function.

\subsubsection*{Related Work}

The question of proper vs. improper learning has been studied in computational
learning theory. While a separation between the two kinds is known, unless $\NP
\neq \RP$, most intersting PAC-learnable classes can be learned using thresholds
of low-degree polynomials.\footnote{For example, the class of $k$-CNF, $k$-term
DNF, decision lists, low-rank decision trees, can all be represented as PTFs.}
In the context of evolvability, strong negative results are known for
distribution-independent evolvability for boolean functions, \eg even the class
of conjunctions is not evolvable. However, it is interesting to study whether
under restricted classes of distributions, whether evolution is possible using
simple restriction classes. Currently, even under (biased) product algorithms,
no evolutionary mechanism is known for the class of disjunctions, except via
Feldman's general reduction from CSQ algorithms. The class of smooth(ed)
distributions may also be a natural starting place for evolvability of simple
concept classes.

\vknote{Talk here about how earlier algorithms can be viewed in this way.}

Learning sparse linear functions is a problem that has been studied under
various names in several fields. Learning the sparsest linear function is
equivalent to finding the sparsest solution to a system of linear equations
(assuming there is no noise in the data). In general this problem is $\NP$-hard
and the approximation factor depends on the norm of the pseudo-inverse of the
matrix\cite{Natarjan:1995}. Thus, some assumption on the distribution seems
necessary. Recovering sparsest solutions is the subject of the body of work
called compressed sensing. The work in this area is too vast to list here;
Donoho et al.~\cite{Donoho:} have a great survey. The evolution based on
optimisation algorithm (Section~\ref{sec:}) is essentially the greedy matching
of Donoho and Troepp, cast in the language of evolvability. Finally, we note
that under the distributional assumption made in the paper, the problem of
sparse linear regression is easy given access to data points. The focus here is
showing that evolutionary mechanisms, without access to data can also succeed,
in a manner that at all times the function computed is also a sparse linear
function.

\subsubsection*{Organization}

In section~\ref{sec:model}, we describe Valiant's evolution model, and describe
the concept classes and class of distributions considered in this paper.
Section~\ref{sec:sparse-algs} contains the evolutionary mechanisms for sparse
linear functions. We conclude in Section~\ref{sec:discussion} with some
discusssion and directions for future work.
