Darwin's theory of evolution through natural selection has been central to the
study of evolution. The modern theory of genetics  -- phenotype -- genotype --
regulatory networks. 

Valiant (2006) introduced a model of evolution to address the question of
complexity that may arise through Darwinian mechanisms. In this model, an
organism is an entity that computes a function of its environment. There is a
hypothetical \emph{ideal function} indicating the best behaviour in every
possible environment. The performance of the organism is measured by how close
the function it computes is to the ideal. Mutations may alter the function an
organism computes. The performance (fitness) forms the basis of natural
selection. The resources allowed are the most generous while remaining feasible,
the mutation mechanism may be any efficient randomised Turing machine, and the
function represented by the organism may be arbitrary as long as it may be
computable by an efficient Turing machine.

Formulated this way, the question of evolvability can be asked in the language
of computational learning theory. For what classes of functions, $C$, can one
expect to find an evolutionary mechanism that gets arbitrarily close to the
ideal, within feasible computational resources? Darwinian selection is
restrictive in the sense that the only feedback received is aggregate over life
experiences. Valiant observed that any feasible evolutionary mechanism could be
simulated in the statistical query framework of Kearns~\cite{Kearns:1994}. In a
remarkable result, Feldman showed that in fact evolvable concept classes are
exactly captured by a restriction of Kearns's model, where the learning algorithm
is only allowed to make \emph{performance queries}, \ie, it produces a hypothesis
and then makes a query that returns the (approximate) loss of that hypothesis under the
distribution.\footnote{Feldman calls these queries correlational statistical
queries, because when working with Boolean functions with range $\{-1, 1\}$, the
performance of any hypothesis is its correlation with the ideal function.} P.
Valiant studied the evolvability of real-valued functions. He showed that
whenever the corresponding weak optimisation problem, \ie, approximately
minimising the expected loss, can be solved by a using a weak-evaluation oracle,
such an algorithm can be converted into an evolutionary
mechanism~\cite{Valiant:2012-real}. This implies that a large of class of
functions -- low degree real polynomials -- can be evolved with respect to any
convex loss function.

Direct evolutionary mechanisms \dots \eanote{Summarize what these are and
what they achieve -- I don't totally understand what ``direct'' means.}
Valiant~\cite{Valiant:2009-evolvability} showed that the class of
disjunctions is evolvable using a simple set of mutations under the uniform
distribution. Kanade, Valiant and Vaughan proposed a simple mechanism for
evolving homogeneous linear separators under radially symmetric
distributions~\cite{KVV:2010-drift}.  Feldman considered a model where the
ideal function is boolean but the representation is real-valued, allowing for
more detailed feedback. He presents an algorithm for evolving large margin
linear separators for a large class of loss functions~\cite{Feldman:2011-LTF}.
P.  Valiant also showed that with very simple mutations, the class of low-degree
polynomials can be evolved with respect to the squared loss~\cite{Valiant:2012-real}.

\todovk{I'd like this paragraph re-worded.}
\todoea{I think much of this paragraph could be deleted.}
Modeling mutations as the outputs of a Turing machine offers appealing
generality that captures a fundamental notion of the evolvable that is not
specific to life on Earth, \eg, requiring that creatures use sequences of DNA
to represent their hypotheses.
At the same time, the more direct algorithms are appealing because \dots
\eanote{Because of their ``naturalness''?}
However, it is difficult to define what constraints ``naturalness'' might place
on mutations.  Our current understanding of biological mutations and the
relationship between genotype -- the representation of a hypothesis --
and phenotype -- its corresponding functional output -- is incredibly limited.
Thus, Valiant~\cite{Valiant:2013-PAC} argued that a temptation to define
\emph{naturalness} should be resisted, \eanote{while observing that natural
systems are constrained?}

In this work, we suggest that the other general aspect in Valiant's model, 
\eanote{What is the first general aspect?} representations being arbitrary
polynomial-time computable functions, may indeed benefit from a closer study of
biology. We first describe a particular class of biological circuits,
\emph{transcription networks}, that motivate our study. We then frame the
question in the language of computational learning theory, summarize our
contributions and discuss related work.

\subsection{Representation in Biology}

\todovk{I don't like this paragraph.. I don't know what to do.}
Biological systems appear to function successfully with greatly restricted
representation classes. For example, one natural restriction is that concept
classes ought to be bounded.
P. Valiant argues that since selection can only act on polynomially many
offspring per generation, evolution can only reasonably approximate bounded
(real) numbers in polynomial time~\cite{Valiant:2012-real}.
We study additional restrictions motivated by physical constraints on
genes and proteins within a cell.

Consider \emph{Escherichia coli}, a single-celled
bacterium that has thrived for millions of years in the digestive systems of
animal hosts; its responses to the environment -- which is dynamic and very
high-dimensional -- are specified in about 4,000 genes~\cite{biology}.
As in other organisms, these genes encode proteins that interact with the
environment, with each other and with genetic material to regulate cellular
function (Figure~\ref{}).
At any particular moment, each protein can physically interact with only
a small number of other molecules -- other proteins, DNA or other molecules,
\eg sugars or toxins.
Consequently, a ubiquitous property of these interaction networks is sparsity
\eanote{that something something\dots}

We focus on transcription networks, which are a specific class of networks of
interacting genes and proteins. For an accessible and mathematical introduction to
transcription networks and other biological circuits, see (Alon~\cite{alon});
below we present a simplified account that motivates this work.

\todoea{Figure: transcription and a simple network.}

In a transcription network, a gene's expression, which is the
quantity of (rate at which) resultant the protein is produced, may be
regulated by a set of proteins called \emph{transcription factors}.
These transcription factors increase or decrease a gene's expression by binding
to regions of DNA close to the gene. 
Thus, physical limitations demand that only a small number of transcription
factors may regulate a single gene; \todoea{most genes have L-U binding sites.}

The number of transcription factors varies from hundreds in a bacterium to
thousands in a human cell.
Some transcription factors are always present in the cell and can be thought of
as representing a \emph{snapshot} of the environment.
For example, when sugar molecules are present in the environment, this may cause
specific transcription factors to increase the production of another protein.
The produced protein may itself be an ``end'' output, such as an enzyme that
catalyzes a metabolic reaction involving the sugar.
Alternatively, it may be another transcription factor that regulate other genes
-- we view this as intermediate computation -- and so on in a cascading
fashion, to yield some desired response.
While transcription networks often include feed-back loops, here for simplicity
we focus on systems that are directed acyclic graphs, and the resulting
computation can be viewed as a circuit.

Another feature of transcription networks is that the time required for
sufficient quantities of protein to be produced is of the same magnitude as
cell-division time. Thus, these circuits are by necessity shallow.
A natural restriction one may place on the representation used by
evolution is that it must be the description of a constant-depth circuit with
constant fan-in gates.\footnote{Other kinds of networks, such
as signaling networks, work by changing shapes of proteins. The fact that these
transformations are rapid may allow for much larger depth.
Fast conformational changes govern how transcription factors directly process 
information from the environment in order to regulate gene expression.
In our example, a sugar molecule binds to a transcription factor and changes
its shape in a way that alters its ability to bind to DNA.}
\todoea{Numbers regarding depth.}
Of course, any such function can depend only on a
constant number of all possible variables. Thus, we consider the class of
functions restricted so that each function depends only on a constant number of
variables.

\subsection{Our Contributions}

First, our contribution is conceptual. We believe the study of evolvability from
a computational standpoint will benefit by understanding the representation
complexity of the organism. Motivated by the previous discussion, in the case of
transcription networks, one may insist that the representation be a
constant depth and fan-in (boolean or arithmetic) circuit. Of course, any
function that can be represented by such a circuit can depend only on a constant
number of input variables.

Second, we show that the class of sparse linear functions, those that depend
only on a constant number of variables, can be evolved using sparse linear
functions, under a large class of smooth distributions, when the performance is
measured using squared error. The number of variables used by the
representations is larger than the number of variables in the \emph{ideal
function} and depends on the \emph{smoothness} parameter of the distribution.
Note that a linear function is represented by a weighted arithmetic circuit with
only one addition gate (alternatively a depth two circuit, with a layer of
multiplication gates and some constant inputs). There is also a natural
trade-off between depth and fan-in. For the precise statement, see
Theorem~\ref{thm:sparse-linear} in Section~\ref{sec:sparse-linear}.

Valiant also proposed a stronger selection mechanism, when natural selection
aggressively selects the (almost) best mutation, than merely a beneficial one --
called evolution by optimisation.  Under stronger conditions on the
distribution, we show that under evolution by optimisation sparse linear
functions can be evolved by representations with the same sparsity. This
algorithm however requires to be initialised, \ie the evolutionary process
begins with the $0$ function.

\subsubsection*{Related Work}

The question of proper vs. improper learning has been studied in computational
learning theory. While a separation between the two kinds is known, unless $\NP
\neq \RP$, most intersting PAC-learnable classes can be learned using thresholds
of low-degree polynomials.\footnote{For example, the class of $k$-CNF, $k$-term
DNF, decision lists, low-rank decision trees, can all be represented as PTFs.}
In the context of evolvability, strong negative results are known for
distribution-independent evolvability for boolean functions, \eg even the class
of conjunctions is not evolvable. However, it is interesting to study whether
under restricted classes of distributions, whether evolution is possible using
simple restriction classes. Currently, even under (biased) product algorithms,
no evolutionary mechanism is known for the class of disjunctions, except via
Feldman's general reduction from CSQ algorithms. The class of smooth(ed)
distributions may also be a natural starting place for evolvability of simple
concept classes.

\vknote{Talk here about how earlier algorithms can be viewed in this way.}

Learning sparse linear functions is a problem that has been studied under
various names in several fields. Learning the sparsest linear function is
equivalent to finding the sparsest solution to a system of linear equations
(assuming there is no noise in the data). In general this problem is $\NP$-hard
and the approximation factor depends on the norm of the pseudo-inverse of the
matrix\cite{Natarjan:1995}. Thus, some assumption on the distribution seems
necessary. Recovering sparsest solutions is the subject of the body of work
called compressed sensing. The work in this area is too vast to list here;
Donoho et al.~\cite{Donoho:} have a great survey. The evolution based on
optimisation algorithm (Section~\ref{sec:}) is essentially the greedy matching
of Donoho and Troepp, cast in the language of evolvability. Finally, we note
that under the distributional assumption made in the paper, the problem of
sparse linear regression is easy given access to data points. The focus here is
showing that evolutionary mechanisms, without access to data can also succeed,
in a manner that at all times the function computed is also a sparse linear
function.

\subsubsection*{Organization}

In section~\ref{sec:model}, we describe Valiant's evolution model, and describe
the concept classes and class of distributions considered in this paper.
Section~\ref{sec:sparse-algs} contains the evolutionary mechanisms for sparse
linear functions. We conclude in Section~\ref{sec:discussion} with some
discusssion and directions for future work.
