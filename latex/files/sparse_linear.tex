In this section, we describe two algorithms for evolving sparse linear
functions. The first evolves the class $\lin^k_{l, u}$ under the class of
$\Delta$-smooth $G$-nice distributions, using the selection rule $\bnsel$. The
second algorithm evolves the class $\lin^k_{0, u}$ under more restricted
distributions, using the selection rule $\optsel$. We first define the notation
used in the rest of this section.\smallskip \\

\noindent{\bf Notation}: $D$ denotes the target distribution over $X =
\reals^n$, $f$ denotes the target (ideal) function. The inner product
$\ip{\cdot}{\cdot}$ and $2$-norm $\ltwonorm{\cdot}$ of functions are with
respect to the distribution $D$. For $S \subseteq [n]$, $f^S$ denotes the best
linear approximation of $f$ using the variables in the set $S$; formally,
%%
\begin{align}
f^S = \underset{w \in \reals^n~:~ w_i = 0 ~\vee~i \in S}\argmin \ltwonorm{f -
w}^2 \label{eqn:artichoke}
\end{align}
%%
Finally, recall that for $w \in \reals^n$, $\NZ(w) = \{i ~|~ w_i \neq 0 \}$ and
$\sparsity(w) = |\NZ(w)|$. A vector $w$ represents a linear function, $x \mapsto
w \cdot x$. The vector $e^i$ has $1$ in coordinate $i$ and $0$ elsewhere and
corresponds to the linear function $x \mapsto x_i$. Thus, in this notation,
$\corr(x_i, x_j) = \ip{e^i}{e^j}/(\ltwonorm{e^i}\ltwonorm{e^j})$. The accuracy
parameter is denoted by $\epsilon$. $[n]$ denotes the set $\{1, \ldots, n\}$.

\subsection{Evolving Sparse Linear Functions Using $\bnsel$}
\label{sec:sparse_linear}

We present a simple algorithm that evolves the class of sparse linear functions,
$\lin^k_{l, u}$, with respect to $\Delta$-smooth $G$-nice distributions (see
Definition~\ref{defn:afghanistan}). The representation class also consists of sparse
linear functions, but with a greater number of non-zero entries than the
\emph{ideal function}.  We also assume that a linear function is represented by
$w \in \reals^n$, where each $w_i$ is a real number. (Handling the issues of
finite precision is standard and is avoided in favour of simplicity.) Define the
parameters $K = 32(k^2/\Delta^4)(u/l)^2$ and $B = 10 uk /\Delta$. Formally, the
representation class is
\[ 
R = \{ w ~|~ \sparsity(w) \leq K, w_i \in [-B, B] \}
\]
The important point to note is that the parameters $K$ and $B$ do not depend on
$n$, the total number of variables.

Next, we define the mutator. Recall that the mutator is a randomized algorithm
that takes as input an element $r \in R$ and accuracy parameter $\epsilon$,
and outputs a multiset $\Neigh(r, \epsilon) \subseteq R$. Here, $\Neigh(r,
\epsilon)$ is populated by $m$ independent draws from the following procedure,
where $m$ will be specified later. Starting from $w \in R$, define the mutated
representation $w^\prime$, output by the mutator, as:
%%
\begin{enumerate}
%
\item (Scaling) With probability $1/3$, choose $\gamma \in [-1, 1]$ uniformly at
random and let $w^\prime = \gamma w$. 
%
\item (Adjusting) With probability $1/3$, do the following.
Pick $i \in \NZ(w) = \{ i~|~ w_i \neq 0 \}$ uniformly at random. Let
$w^\prime$ denote the mutated representation, where $w^\prime_j = w_j$ for $j
\neq i$, and choose $w^\prime_i \in [-B, B]$ uniformly at random.
%
\item With the remaining $1/3$ probability, do the following:
\begin{enumerate}
\item (Swapping) If $|\NZ(w)| = K$, choose $i_1 \in \NZ(w)$ uniformly at random.
Then, choose $i_2 \in [n] \setminus \NZ(w)$ uniformly at random. Let $w^\prime$
be the mutated representation, where $w_j^\prime = w_j$ for $j \neq i_1, i_2$.
Set $w_{i_1}^\prime = 0$ and choose $w_{i_2}^\prime \in [-B, B]$ uniformly at
random. In this case, $\sparsity(w^\prime) = \sparsity(w) = K$
with probability $1$, and hence $w^\prime \in R$.
\item (Adding) If $|\NZ(w)| < K$, choose $i \in [n] \setminus \NZ(w)$ uniformly
at random. Let $w^\prime$ be the mutated representation, where $w_j^\prime =
w_j$ for $j \neq i$, and choose $w^\prime_i \in [-B, B]$ uniformly at random.
\end{enumerate}
\end{enumerate}

Recall that $f \in \lin^k_{l, u}$ denotes the target (ideal) function, $D$ is
the underlying distribution that is $\Delta$-smooth $G$-nice (see
Definition~\ref{defn:afghanistan}). Since we are working with the squared loss
metric, $\ell(y^\prime, y) = (y^\prime - y)^2$, the expected loss for any $w \in
R$ is given by $\loss_{f, D}(w) = \ltwonorm{f - w}^2 = \ip{f - w}{f - w}$.  We
will show that for any $w \in R$, if $\ltwonorm{f - w}^2 > \epsilon$, with
non-negligible (inverse polynomial) probability, the above procedure produces a
mutation $w^\prime$ that decreases the expected loss by at least some inverse
polynomial amount. Thus, by setting the size of the neighbourhood $m$ large
enough, we can guarantee that with high probability there will always exist a
beneficial mutation.
%
%such that $\loss_{f, D}(w^\prime) = \ltwonorm{f - w^\prime}
%\leq \loss_{f, D}(w) - b$, for some inverse polynomial $b$.

To simplify notation, let $S = \NZ(w)$. Recall that $f^S$ denotes the best
approximation to $f$ using variables in the set $S$; thus, $\ltwonorm{f - w}^2 =
\ltwonorm{f - f^S}^2 + \ltwonorm{f^S - w}^2$. At a high level, the argument for
proving the success of an evolutionary mechanism is as follows: If
$\ltwonorm{f^S - w}^2$ is large, then a mutation of the type ``scaling'' or
``adjusting'' will get $w$ closer to $f^S$, reducing the expected loss. (The
role of ``scaling'' mutations is primarily to ensure that the representations
remain bounded.) If $\ltwonorm{f^S - w}^2$ is small and $S \neq \NZ(f)$, there
must be a variable in $\NZ(f) \setminus S$, that when added to $w$ (possibly by
swapping), reduces the expected loss. Thus, as long as the representation is far
from the evolutionary target, a \emph{beneficial} mutation is produced with high
probability.

More formally, let $w^\prime$ denote a random mutation produced as a result of
the procedure described above.  We will establish the desired result by proving
the following claims.
%%
\begin{claim} \label{claim:apple} If $\ltwonorm{w} \geq 2 \ltwonorm{f^S}$, then
with probability at least $1/12$, $\loss_{f, D}(w^\prime) \leq \loss_{f, D}(w) -
\ltwonorm{w - f^S}^2/12$. In particular, a ``scaling'' type mutation achieves
this. \end{claim}
%%
\begin{claim} \label{claim:banana} When $\ltwonorm{w} \leq 2 \ltwonorm{f^S}$,
then with probability at least $\Delta \ltwonorm{f^S - w}/(6K^2 B)$, there
exists a mutation that decreases the expected loss by at least $3
\Delta^2\ltwonorm{f^S - w}^2/(4|S|^2)$. In particular, an ``adjusting'' type
mutation achieves this. \end{claim}
%%
\begin{claim} \label{claim:cantaloupe} When $\ltwonorm{f^S - w} \leq
l^2\Delta^2/(4KB)$, but $\NZ(f) \not\subseteq S$, then with probability
at least $\Delta \ltwonorm{f - w}/(6KBnk)$, there exists a mutation that
decreases the expected loss by at least $\Delta^2 \ltwonorm{f-w}^2/(16k^2)$.
\end{claim}

\noindent Note that when $\NZ(f) \subseteq S$, then $f^S = f$. Thus, in this case
when $\loss_{f, D}(w) = \ltwonorm{f^S - w}^2 \leq \epsilon$, the evolutionary
algorithm has succeeded. \medskip 

The proofs of the above Claims are provided in
Appendix~\ref{app:sparse_linear}. We now prove our main result using the
above claims.
% \vknote{I think we'll eventually structure this by saying -- the proofs of the
% claim are in Appendix Hugh. Then, we'll have a short proof showing that if the
% Claims are true, evolution indeed succeeds.}
% 

\begin{theorem} Let $\Dists$ be the class of $\Delta$-smooth $G$-nice
distributions over $\reals^n$ (Definition~\ref{defn:afghanistan}). Then the class
$\lin^k_{l, u}$ is evolvable using the representation class $\lin^K_{0, B}$,
where $K = O((k/\Delta)^4 (u/l)^2)$ and $B = O(uk/\Delta)$, using the mutation
algorithm described in this section, the selection rule $\bnsel$.
Furthermore, the following are true:
\begin{enumerate}
\item The number of generations required is $O()$ and is independent of $n$. \todoea{Fill in}
\item The size polynomial $s$, depends polylogarithmically on $n$, and
polynomially on the remaining parameters. \todoea{Fill in}
\end{enumerate}
\label{thm:sparse_linear} \end{theorem}
\begin{proof}
The mutator is as described in this section. Let $p = \min\{1/12,
l^2\Delta^3/(24K^3B^2), \sqrt{\epsilon}/(6KBnk)\}$, and let $\alpha =
\sqrt{\epsilon} \min\{(l^4 \Delta^4)/(192 K^2B^2), (3 l^4 \Delta^6)/(64K^4B^2),
(\epsilon\Delta^2)/(16k^2)\}$. Now, by Claims~\ref{claim:apple},
\ref{claim:banana} and \ref{claim:cantaloupe}, if $\ltwonorm{f - w}^2 \geq
\epsilon$, then the mutator outputs a mutation that decreases the squared loss
by $\alpha$ with probability at least $p$.

Recall that for $w \in R$, $\sum_{i} w_i^2 \leq KB^2$. By assumption it is the
case that for any $x$ in the support of $D$, $\sum_{i} x_i^2 \leq G^2$. Clearly,
since $B > u$ and $K > k$, $f \in R$. Thus, $\ell(f(x), w(x)) \leq 4KB^2G^2$ for
$w \in R$ and $x$ in the support of $D$. Let $g = 20 KB^2G^2/\alpha$, $m =
(1/p)\ln(2g/\epsilon)$, $t = 3\alpha/5$  and $s = (800 KB^2G^2/\alpha^2)
\ln(4mg/\epsilon)$. Here $s$ is the size function and $t$ is the tolerance
function.

By our choice of $s$, for a fixed representation $w \in R$, if $\hat{\loss}_{f,
D}(w) = (1/s) \sum_{i = 1}^s \ell(w(x^i), f(x^i))$, then the probability that
$|\loss_{f, D}(w) - \hat{\loss}_{f, D}(w)| \geq \alpha/5$ is at most
$\epsilon/(2mg)$. Thus, we imagine that the evolution algorithm runs for $g$
generations using the mutator defined and $\bnsel$ as the selection rule. Then,
for each representation $w$ in the neighbourhood, $\Neigh(r, \epsilon)$ (of size
$m$), for $g$ generations we assume that $|\loss_{f, D}(w) - \hat{\loss}_{f,
D}(w)| \leq \alpha/5$, except with probability $\epsilon/2$ (by a straighforward
union bound). Similarly, by a union bound, each neighbourhood for $g$ generation
contains a mutation that decreases expected loss by at least $\alpha$, except
with probability $\epsilon/2$. Allowing a failure probability of $\epsilon$, we
consider the event that no failure occurs. 

This implies that for $t = \alpha/3$, for $g$ generations, there always exists
some $w^\prime$ in the neighbourhood of the current representation $w$, such
that $\hat{\loss}_{f, D}(w^\prime) \leq \hat{\loss}_{f, D}(w) - t$. Thus, a
beneficial mutation always exists and will be picked. Also, since $t = 3
\alpha/5$, if $w^\prime$ is such that $\hat{\loss}_{f, D}(w^\prime) \leq
\hat{\loss}_{f, D}(w) - t$, it must be the case that $\loss_{f, D}(w^\prime) \leq
\loss_{f, D}(w) - \alpha/5$. Thus, the expected loss decreases by at least
$\alpha/5$ each generation. Since $4KB^2G^2$ is an upper bound on the expected
loss, it must be the case that evolution succeeds in $g = 20 KB^2G^2/\alpha$
generations.

The statement of the theorem now follows immediately.
Notice that $p$ is inverse polynomial in $n$, and hence $s$ has
polylogarithmic dependence on $n$. $g$ has no dependence on $n$.
\end{proof}

\begin{remark} We note that the same evolutionary process works when evolving
the class $\lin^k_{0, u}$, as long as the sparsity $K$ of the representation
class is allowed polynomial dependence on $1/\epsilon$, the accuracy parameter.
This is consistent with the notion of attribute-efficiency, where the goal is
that the information complexity should be polylogarithmic in the number of
attributes.
\end{remark}
