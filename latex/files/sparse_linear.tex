We present a simple algorithm that evolves the class of sparse linear functions,
$\lin^k_{l, u}$.  The representation class also consists of sparse linear
functions, but with greater number of non-zero entries than the \emph{ideal
function}. We will define the parameters, $K$ and $B$ in the definition below
later. We also assume that a linear function is represented by $w \in \reals^n$,
where each $w_i$ is a real number. (Handling the issues of finite precision is
standard and is avoided in favour of simplicity.) Formally, the representation
class is,
\[ 
R = \{ w \mapsto w \cdot x ~|~ \lznorm{w} \leq K, w_i \in [-B, B] \}
\]

Next, we define the mutator. Recall that the mutator is a randomized algorithm
that takes as input an element, $r \in R$, the accuracy parameter, $\epsilon$,
and outputs a multiset $\Neigh(r, \epsilon) \subseteq R$. Here, $\Neigh(r,
\epsilon)$ is populated by $m$ independent draws from the following procedure,
where $m$ will be specified later. For $w \in R$, define the mutated
representation $w^\prime$, output by the mutator as:
%%
\begin{enumerate}
%
\todovk{I don't think we need this mutation 1, we may be able to get rid of it.}
%
\item With probability $1/2$ do nothing, set $w^\prime = w$. %
\item (Scaling) With probability $1/6$, choose $\gamma \in [-1, 1]$ uniformly at
random and let $w^\prime = \gamma w$. 
%
\item (Adjusting) With probability $1/6$, do the following. Recall that $\NZ(w)
= \{ i~|~ w_i \neq 0 \}$. Pick $i \in \NZ(w)$ uniformly at random. Let
$w^\prime$ denote the mutated representation, where $w^\prime_j = w_j$ for $j
\neq i$, $w^\prime_i \in [-B, B]$ uniformly at random. 
%
\item With the remaining $1/6$th probability, do the following:
\begin{enumerate}
\item (Swapping) If $|\NZ(w)| = K$, choose $i_1 \in \NZ(w)$ uniformly at random.
Then, choose $i_2 \in [n] \setminus \NZ(w)$ uniformly at random. Let $w^\prime$
be the mutated representation, where $w_j^\prime = w_j$ for $j \neq i_1, i_2$.
Set $w_{i_1}^\prime = 0$ and choose $w_{i_2}^\prime$ uniformly from the range
$[-B, B]$. Recall that in this case, $\sparsity(w^\prime) = \sparsity(w) = K$
with probability $1$, and that $w^\prime \in R$.
\item (Adding) If $|\NZ(w)| < K$, choose $i \in [n] \setminus \NZ(w)$ uniformly
at random. Let $w^\prime$ be the mutated representation, where $w_j^\prime =
w_j$ for $j \neq i$, $w^\prime_i$ is chosen uniformly in $[-B, B]$. 
\end{enumerate}
\end{enumerate}

In the ensuing discussion, we fix the following notation, $f \in C^k_{l, u}$ is
the target linear function; the underlying distribution is $D$ which is a
$\Delta$-smooth bounded distribution (see Section~\ref{sec:notation-class}). Recall
that for linear functions, $w, w^\prime$, we have the inner product
$\ip{w}{w^\prime} = \E_{x \sim D}[(w \cdot x) (w^\prime \cdot x)]$, which also
defines the norm $\ltwonorm{w} = \ip{w}{w}$.  We will show that for any $w \in
R$, if $\loss_{f, D}(w) = \ltwonorm{f - w}^2 > \epsilon$, with non-negligible
(inverse polynomial) probability, the above procedure produces a mutation
$w^\prime$, such that $\loss_{f, D}(w^\prime) = \ltwonorm{f - w^\prime} \leq
\loss_{f, D}(w) - b$, for $b = $. \todovk{Missing definition of $b$ add this.}

To simplify notation, let $S = \NZ(w)$. Let $f^S$ denote best approximation to
$f$ using variables in the set $S$. Formally, \todovk{Try to get Latex to put
the stuff under argmin like a math operator.}
\[ 
f^S = \argmin_{w \in R ~:~	w_i = 0 ~\vee~ i \in S} \ltwonorm{f- w}^2 
\]

Let $w^\prime$ denote a random mutation produced as a result of the procedure
described above.  We will establish the desired result by proving the following
claims.
%%
\begin{claim} \label{claim:apple} If $\ltwonorm{w} \geq 2 \ltwonorm{f^S}$, then
with probability at least $1/24$, $\loss_{f, D}(w^\prime) \leq \loss_{f, D}(w) -
\ltwonorm{w - f^S}^2/12$. In particular, a ``scaling'' type mutation achieves
this. \end{claim}
%%
\begin{claim} \label{claim:banana} When $\ltwonorm{w} \leq 2 \ltwonorm{f^S}$,
then with probability at least $\ltwonorm{f^S - w}/(2 \Delta |S| B)$, there
exists a mutation that decreases the $\lerror$ by at least $3\ltwonorm{f^S -
w}^2/(4|S|^2 \Delta^2)$. In particular, an ``adjusting'' type mutation achieves
this. \end{claim}
%%
\begin{claim} \label{claim:cantaloupe} When $\ltwonorm{w - f^S} \leq \epsilon$, but
$\sparseset{f} \not\subseteq S$, then with probability at least \vknote{FILL
THIS}, there exists a mutation that decreases
$\lerror$ by at least \vknote{FILLTHIS}. \end{claim}

Finally, note that when $\NZ(f) \subseteq S$, then $f^S = f$. Thus, in this case
when $\loss_{f, D}(w) = \ltwonorm{f^S - w}^2 \leq \epsilon$, the evolutionary
algorithm has succeeded. \medskip 

\vknote{I think we'll eventually structure this by saying -- the proofs of the
claim are in Appendix Hugh. Then, we'll have a short proof showing that if the
Claims are true, evolution indeed succeeds.}

\begin{proof}[Proof of Claim \ref{claim:apple}]: We show that in this case, a ``scaling''
mutation achieves the desired result. Restricted to the direction $w$, the best
approximation to $f^S$ is $\frac{\ip{w}{f^S}}{\ltwonorm{w}^2}w$. We have that, 
%%
\[
\left\Vert \frac{\ip{w}{f^S}}{\ltwonorm{w}^2} w \right\Vert \leq
\ltwonorm{f^S} \leq \frac{\ltwonorm{w}}{2}
\]
%%
Hence, if $\ip{w}{f^S} > 0$ for $\gamma \in [1/4, 3/4]$ (and similarly if
$\ip{w}{f^S} < 0$ for $\gamma \in [-3/4, -1/4]$ ), we have that,
%%
\begin{align*}
\ltwonorm{\gamma w - f^S}^2 &= \ltwonorm{w- f^S}^2 - 2 (1 - \gamma) \ip{w -
f^S}{w} + (1 - \gamma)^2 \ltwonorm{w}^2 \\
&\leq \ltwonorm{w - f^S}^2 - (1 - \gamma) \ltwonorm{w}^2 + (1 - \gamma)^2
\ltwonorm{w}^2 \\
&\leq \ltwonorm{w - f^S}^2 - (\gamma - \gamma^2) \ltwonorm{w}^2
\intertext{Finally, by observing that for $\gamma \in [1/4, 3/4]$, $\gamma -
\gamma^2 \geq 3/16$ and that by triangle inequality, $\ltwonorm{w} \geq
(2/3)\ltwonorm{w - f^S}$ when $\ltwonorm{w} \geq 2 \ltwonorm{f^S}$, we get that}
%
\loss_{f, D}(w^\prime) = \ltwonorm{\gamma w - f^S}^2 &\leq \ltwonorm{w - f^S}^2 - \frac{1}{12}
\ltwonorm{w - f^S}^2
\end{align*}
A correct value of $\gamma$ is chosen with probability at least $1/4$, and
combined with the probability of choosing a scaling mutation we get the desired
result.
\end{proof}

\begin{proof}[Proof of Claim~\ref{claim:banana}] Here, we appeal to a mutation
that adjusts the relative weights of the variables within the set $S = \NZ(w)$.
Consider the vector, $f^S - w$, and note that $\NZ(f^S -w ) \subseteq S$. Let
$r^S = f^S - w$ denote the residual within the space spanned by $S$. Then
consider, 
\begin{align*}
\ip{r^S}{r^S} &= \sum_{i \in S} r^S_i \ip{e^i}{r^S}
\intertext{Here, $e^i$ is the unit vector representing the linear function $x
\mapsto e^i \cdot x = x_i$. Therefore, there must exist $i$ for which the
following is true,}
r^S_i\ip{e^i}{r^S} &\geq \frac{\ip{r^S}{r^S}}{|S|}
\intertext{We appeal to Lemma~\ref{lemma:amsterdam} (part 1), which implies that
$|r^S_i| \leq \sqrt{\ip{r^S}{r^S}/\Delta^2} = \ltwonorm{r^S}{\Delta}$ to
conclude that,}
|\ip{e^i}{r^S}| &\geq \frac{\ltwonorm{r^S}}{|S| \Delta}
\end{align*}
Let $\beta = \frac{\ip{e^i}{r^S}}{\ltwonorm{e^i}^2}$. Then, suppose $w^\prime =
w + \gamma e^i$ for $\gamma \in [\beta - |\beta|/2, \beta +  |\beta|/2]$. We
then have,
\begin{align*}
\ltwonorm{f^S - (w + \gamma e^i)}^2 &= \ltwonorm{w - f^S}^2 - 2 \gamma \ip{f^S -
w}{e^i} + \gamma^2 \ltwonorm{e^i}^2
\intertext{Note that $f^S - w = r^S$ and that $\gamma$ and $\ip{e^i}{r^S}$ and
$\gamma$ have the same sign. This combined with (\ref{eqn:andora}) above and the fact
that $\ltwonorm{e^i} \leq 1$, gives,}
\ltwonorm{f^s - (w + \gamma e^i)}^2 &\leq \ltwonorm{w - f^S}^s - (2
|\gamma||\beta| + \gamma^2) \ltwonorm{e^i}^2
\intertext{Finally, note that for $|\gamma| \in [|\beta|/2, 3|\beta|/2]$, $- 2
|\gamma| |\beta| + |\beta|^2 \leq - 3 |\beta|^2/4$, hence the
above equation yields}
\ltwonorm{f^S - (w + \gamma e^i)}^2 &\leq \ltwonorm{w - f^S}^2 - \frac{3}{4}
\beta^2 \ltwonorm{e^i}^2 \leq \ltwonorm{w - f^S}^2 - \frac{3}{4}
\frac{\ltwonorm{f^s - w}^2}{|S|^2 \Delta^2}
\end{align*}

Note that the a suitable mutation $w^\prime = w + \gamma e^i$ is obtained with
probability at least $|\beta|/(2B)$. This, however relies on the fact that $w_i
+ \gamma \in [-B, B]$. This can be ensured by choosing that the parameter $B$
suitably. Note that that $\ltwonorm{w} \leq 2 \ltwonorm{f^S} \leq 2
\ltwonorm{f}$ (the last part is because $f^S$ is a projection of $f$ onto a
lower dimensional space). Thus, by Lemma~\ref{lemma:amsterdam} (part 1), $w_i
\leq 2 \ltwonorm{f} / \Delta$. Also, by $|\beta| =
|\ip{e^i}{r^S}|/\ltwonorm{e^i}^2 \leq \ltwonorm{r^S}/\ltwonorm{e^i} \leq 3
\ltwonorm{f}/\Delta$. Thus, if $B > 10 \ltwonorm{f}/ \Delta$, the mutation will
be a feasible one. Note that the maximum value of $\ltwonorm{f}$ for $f \in
\lin^k_{l, u}$ is $ku$. Thus, this condition is satisfied for the choice of
$\beta$ and the condition is satisfied. This completes the proof of
Claim~\ref{claim:banana}.
\end{proof}

\begin{proof}[Proof of Claim~\ref{claim:cantaloupe}] Finally, we show that if
$\ltwonorm{f^S - w}$ is very small, but $\NZ(f) \not\subseteq S$, then it must
be the case that a ``swapping'' or ``adding'' mutation is beneficial. We focus
on the swapping case, \ie when $|S| = K$; the adding step is a special case of
this. First, we observe that if there exists $i \in \NZ(f)$ such that $i \not\in
S$, then by using Lemma~\ref{lemma:amsterdam} (part 1), it must be the case that
$\ltwonorm{f - w}^2 \geq f_i^2 \Delta^2 \geq l^2 \Delta^2$.

Also, Lemma~\ref{lemma:amsterdam} (part 2) implies that there exists $i \in S$,
such that $w^2_i \leq \ltwonorm{w}^2/(K \Delta^2)$. Finally, consider the
following:
\begin{align*}
\ip{f - w}{f - w} &=  \sum_{i \in \NZ(f) \setminus S} (f_i - w_i) \ip{e^i}{f -
w} + \sum_{i \in S} (f_i - w_i) \ip{e^i}{f - w}
\intertext{Note that, for all $i \in S$, $\ip{e^i}{f - f^S} = 0$, since $f^S$ is
the projection of $f$ on to the space spanned by the variables in $S$. Hence,
$\ip{e^i}{f - w} = \ip{e^i}{f^S - w} \leq \ltwonorm{f^S - w}$. Using this we
get, }
\ip{f - w}{f - w} &\leq \sum_{i \ in \NZ(f) \setminus S} (f_i - w_i) \ip{e^i}{f
- w} + K \ltwonorm{f - w} \sum_{i \in S} |f^i - w^i| 
\intertext{Now, even by a very crude estimate, $|f^i - w^i| \leq 2 B$, and hence
by the condition in the statement of Claim~\ref{claim:cantaloupe} together with
the observation that $\ltwonorm{f - w}^2 \geq l^2 \Delta^2$, we have that,}
\frac{1}{2}\ip{f - w}{f - w} &\leq \sum_{i \in \NZ(f) \setminus S} (f_i - w_i)
\ip{e^i}{f - w}
\end{align*}

\end{proof}
Here, we assume that $\ltwonorm{w^*_S - w}^2 \leq \epsilon$ (because otherwise
we konw that a beneficial mutation already exists with non-negligible
probability). Using Lemma~\ref{lemma:at-least-one-small}, we know that there
exists $i \in S$, such that $|w_i| \leq \ltwonorm{w}/(\Delta \sqrt{K}$. Consider
the following:

\begin{align*}
\ltwonorm{w^* - w}^2 &= \ip{w^* - w}{w^* - w}
&= \ip{w^* - w^*_S}{w^* - w} + \ip{w^*S - w}{w^* - w}
\intertext{Since $\ltwonorm{w^*S - w} \leq \sqrt{\epsilon}$ and $\ltwonorm{w^* -
w} \leq 3 \ltwonorm{w^*}$, we get that for $\epsilon$ small enough,}
\ltwonorm{w^* - w}^2 &\leq 2 \ip{w^* - w^*_S}{w^* - w}
\intertext{Next, we observe that $\ip{w^* - w^*_S}{e^i} = 0$ for every $i \in
S$, since by definition of $w^*_S$ is the projection of $w^*$ in the space
spanned by the variables in the set $S$. Thus, it must be the case that there
exists some $j \in \sparseset(w^*) \setminus S$, such that}
w^*_j \ip{w^* - w}{e_j} &\geq \frac{\ltwonorm{w^* - w}^2}{2k} \geq \frac{\Delta^2
l^2}{2k}
\intertext{Let $\beta = \ip{w^* - w}{e_j}/\ltwonorm{e_j}^2$. As argued in the
proof of Claim $B$, if $\gamma \in [\beta/2, 3\beta/2$ causes $\lerror$ to drop
by at least $3 \beta^2/ 4$. Also in this case, we need to verify that $\beta \in
[-B, B]$.}
\end{align*}
