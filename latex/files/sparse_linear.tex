In this section, we describe two evolutionary algorithms for evolving sparse
linear functions. The first evolves the class $\lin^k_{l, u}$ under the class of
$\Delta$-smooth $G$-nice distributions (Defn.~\ref{defn:afghanistan}), using the
selection rule $\bnsel$. The second evolves the class $\lin^k_{0, u}$ under the
more restricted class of $(1/2k)$-incoherent $(\Delta, G)$-nice distributions
(Defn.~\ref{defn:bhutan}) distributions, using the selection rule $\optsel$. We
first define the notation used in the rest of this section.\smallskip \\

\noindent{\bf Notation}: $D$ denotes the target distribution over $X =
\reals^n$, $f$ denotes the ideal (target) function. The inner product
$\ip{\cdot}{\cdot}$ and $2$-norm $\ltwonorm{\cdot}$ of functions are with
respect to the distribution $D$. $[n]$ denotes the set $\{1, \ldots, n\}$. For
$S \subseteq [n]$, $f^S$ denotes the best linear approximation of $f$ using the
variables in the set $S$; formally,
%%
\begin{align}
f^S = \underset{w \in \reals^n~:~ w_i = 0 ~\vee~i \in S}\argmin \ltwonorm{f -
w}^2 \label{eqn:artichoke}
\end{align}
%%
Finally, recall that for $w \in \reals^n$, $\NZ(w) = \{i ~|~ w_i \neq 0 \}$ and
$\sparsity(w) = |\NZ(w)|$. A vector $w$ represents a linear function, $x \mapsto
w \cdot x$. The vector $e^i$ has $1$ in coordinate $i$ and $0$ elsewhere and
corresponds to the linear function $x \mapsto x_i$. Thus, in this notation,
$\corr(x_i, x_j) = \ip{e^i}{e^j}/(\ltwonorm{e^i}\ltwonorm{e^j})$. The accuracy
parameter is denoted by $\epsilon$. 

\subsection{Evolving Sparse Linear Functions Using $\bnsel$}
\label{sec:sparse_linear}

We present a simple mechanism that evolves the class of sparse linear functions,
$\lin^k_{l, u}$, with respect to $\Delta$-smooth $G$-nice distributions (see
Defn.~\ref{defn:afghanistan}). The representation class also consists of
sparse linear functions, but with a greater number of non-zero entries than the
\emph{ideal function}. We also assume that a linear function is represented by
$w \in \reals^n$, where each $w_i$ is a real number. (Handling the issues of
finite precision is standard and is avoided in favour of simplicity.) Define the
parameters $K = 5184(k/\Delta)^4(u/l)^2$ and $B = 10 uk /\Delta$. Formally, the
representation class is
\[ 
R = \{ w ~|~ \sparsity(w) \leq K, w_i \in [-B, B] \}
\]
The important point to note is that the parameters $K$ and $B$ do not depend on
$n$, the total number of variables.

Next, we define the mutator. Recall that the mutator is a randomized algorithm
that takes as input an element $r \in R$ and accuracy parameter $\epsilon$, and
outputs a multiset $\Neigh(r, \epsilon) \subseteq R$. Here, $\Neigh(r,
\epsilon)$ is populated by $m$ independent draws from the following procedure,
where $m$ will be specified later (see proof of Thm. ~\ref{thm:sparse_linear}).
Starting from $w \in R$, define the mutated representation $w^\prime$, output by
the mutator, as:
%%
\begin{enumerate}
%
\item {\em Scaling}: With probability $1/3$, choose $\gamma \in [-1, 1]$ uniformly at
random and let $w^\prime = \gamma w$. 
%
\item {\em Adjusting}: With probability $1/3$, do the following.  Pick $i \in
\NZ(w) = \{ i~|~ w_i \neq 0 \}$ uniformly at random. Let $w^\prime$ denote the
mutated representation, where $w^\prime_j = w_j$ for $j \neq i$, and choose
$w^\prime_i \in [-B, B]$ uniformly at random.
%
\item With the remaining $1/3$ probability, do the following:
\begin{enumerate}
\item {\em Swapping}: If $|\NZ(w)| = K$, choose $i_1 \in \NZ(w)$ uniformly at random.
Then, choose $i_2 \in [n] \setminus \NZ(w)$ uniformly at random. Let $w^\prime$
be the mutated representation, where $w_j^\prime = w_j$ for $j \neq i_1, i_2$.
Set $w_{i_1}^\prime = 0$ and choose $w_{i_2}^\prime \in [-B, B]$ uniformly at
random. In this case, $\sparsity(w^\prime) = \sparsity(w) = K$
with probability $1$, and hence $w^\prime \in R$.
\item {\em Adding}: If $|\NZ(w)| < K$, choose $i \in [n] \setminus \NZ(w)$ uniformly
at random. Let $w^\prime$ be the mutated representation, where $w_j^\prime =
w_j$ for $j \neq i$, and choose $w^\prime_i \in [-B, B]$ uniformly at random.
\end{enumerate}
\end{enumerate}

Recall that $f \in \lin^k_{l, u}$ denotes the ideal (ideal) function, $D$ is the
underlying distribution that is $\Delta$-smooth $G$-nice (see
Defn.~\ref{defn:afghanistan}). Since we are working with the squared loss
metric, $\ell(y^\prime, y) = (y^\prime - y)^2$, the expected loss for any $w \in
R$ is given by $\loss_{f, D}(w) = \ltwonorm{f - w}^2 = \ip{f - w}{f - w}$.  We
will show that for any $w \in R$, if $\ltwonorm{f - w}^2 > \epsilon$, with
non-negligible (inverse polynomial) probability, the above procedure produces a
mutation $w^\prime$ that decreases the expected loss by at least some inverse
polynomial amount. Thus, by setting the size of the neighbourhood $m$ large
enough, we can guarantee that with high probability there will always exist a
beneficial mutation.
%
%such that $\loss_{f, D}(w^\prime) = \ltwonorm{f - w^\prime}
%\leq \loss_{f, D}(w) - b$, for some inverse polynomial $b$.

To simplify notation, let $S = \NZ(w)$. Recall that $f^S$ denotes the best
approximation to $f$ using variables in the set $S$; thus, $\ltwonorm{f - w}^2 =
\ltwonorm{f - f^S}^2 + \ltwonorm{f^S - w}^2$. At a high level, the argument for
proving the success of an evolutionary mechanism is as follows: If
$\ltwonorm{f^S - w}^2$ is large, then a mutation of the type ``scaling'' or
``adjusting'' will get $w$ closer to $f^S$, reducing the expected loss. (The
role of ``scaling'' mutations is primarily to ensure that the representations
remain bounded.) If $\ltwonorm{f^S - w}^2$ is small and $S \neq \NZ(f)$, there
must be a variable in $\NZ(f) \setminus S$, that when added to $w$ (possibly by
swapping), reduces the expected loss. Thus, as long as the representation is far
from the evolutionary target, a \emph{beneficial} mutation is produced with high
probability.

More formally, let $w^\prime$ denote a random mutation produced as a result of
the procedure described above.  We will establish the desired result by proving
the following claims.
%%
\begin{claim} \label{claim:apple} If $\ltwonorm{w} \geq 2 \ltwonorm{f^S}$, then
with probability at least $1/12$, $\loss_{f, D}(w^\prime) \leq \loss_{f, D}(w) -
\ltwonorm{w - f^S}^2/12$. In particular, a ``scaling'' type mutation achieves
this. \end{claim}
%%
\begin{claim} \label{claim:banana} When $\ltwonorm{w} \leq 2 \ltwonorm{f^S}$,
then with probability at least $\Delta \ltwonorm{f^S - w}/(6K^2 B)$, there
exists a mutation that decreases the expected loss by at least $3
\Delta^2\ltwonorm{f^S - w}^2/(4|S|^2)$. In particular, an ``adjusting'' type
mutation achieves this. \end{claim}
%%
\begin{claim} \label{claim:cantaloupe} When $\ltwonorm{f^S - w} \leq
l^2\Delta^2/(4KB)$, but $\NZ(f) \not\subseteq S$, then with probability
at least $\Delta \ltwonorm{f - w}/(6KBnk)$, there exists a mutation that
decreases the expected loss by at least $\Delta^2 \ltwonorm{f-w}^2/(16k^2)$.
\end{claim}

\noindent Note that when $\NZ(f) \subseteq S$, then $f^S = f$. Thus, in this case
when $\loss_{f, D}(w) = \ltwonorm{f^S - w}^2 \leq \epsilon$, the evolutionary
algorithm has succeeded. \medskip 

The proofs of the above Claims are provided in
Appendix~\ref{app:sparse_linear}. We now prove our main result using the
above claims.

\begin{theorem} Let $\Dists$ be the class of $\Delta$-smooth $G$-nice
distributions over $\reals^n$ (Defn.~\ref{defn:afghanistan}). Then the
class $\lin^k_{l, u}$ is evolvable with respect to $\Dists$, using the
representation class $\lin^K_{0, B}$, where $K = O((k/\Delta)^4 (u/l)^2)$ and $B
= O(uk/\Delta)$, using the mutation algorithm described in this section, the
selection rule $\bnsel$.  Furthermore, the following are true:
%%
\begin{enumerate}
%
\item The number of generations required is polynomial in $(u/l)$, $1/\epsilon$,
$1/\Delta$, and is independent of $n$, the total number of attributes. 
%
\item The size function $s$, the number of points used to calculate empirical
losses, depends polylogarithmically on $n$, and
polynomially on the remaining parameters. 
%
\end{enumerate}
\label{thm:sparse_linear} \end{theorem}
\begin{proof}
The mutator is as described in this section. Let 
%%
\[ p = \min\left\{\frac{1}{12}, \frac{l^2\Delta^3}{24K^3B^2}, \frac{\Delta
\sqrt{\epsilon}}{6KBnk}\right\},\] 
%%
and let 
%%
\[ \alpha = \min\left\{\frac{l^4 \Delta^4}{192 K^2B^2}, \frac{3 l^4
\Delta^6}{64K^4B^2}, \frac{\epsilon\Delta^2}{16k^2}\right\}.\] 
%%
Now, by Claims~\ref{claim:apple}, \ref{claim:banana} and \ref{claim:cantaloupe},
if $\ltwonorm{f - w}^2 \geq \epsilon$, then the mutator outputs a mutation that
decreases the squared loss by $\alpha$ with probability at least $p$.

Recall that $K = 5184 (k/\Delta)^4 (u/l)^2$ and $B = 10uk/\Delta$.  Now, let $g
= 20 K G^2 B^2/\alpha$ (recall that $G^2$ is the bound on $\sum_{i} x_i^2$ for
$x$ in the support of the distribution). We will show that evolution succeeds in
at most $g$ generations. Note, that $g$ has no dependence on $n$, the number of
attributes and polynomial dependence on the remaining parameters. Define $m =
p^{-1} \ln(2g /\epsilon)$, and at each time-step we have that $|\Neigh(w,
\epsilon)| = m$.  Note that together with the observation above, this implies
that except with probability $\epsilon/2$, for $1 \leq i \leq g$, if $w^i$ is
the representation at time-step $i$, $\Neigh(w^i, \epsilon)$ contains a mutation
that decreases the loss by at least $\alpha$, if $\loss_{f, D}(w^i) \geq
\epsilon$. 

Now, let $t = 3\alpha / 5$ be the \emph{tolerance function}, set $\tau =
\alpha/5$ and let $s = (200 KG^2B^2/\alpha^2) \ln(4mg/\epsilon)$ be the
\emph{size function}. Note that, $\sum_{i} w_i^2 \leq KB^2$ for $w \in R$ (this
also holds for $f$, since $k < K$ and $u < B$). If $\langle x^i \rangle_{i=1}^s$
is an i.i.d. sample drawn from $d$, for each $\bar{w}$ of the $mg$
representations that may be considered in the neighbourhoods for the first $g$
time-steps, using~(\ref{eqn:concentration})\vknote{FILL THIS}, it holds that
$|\hat{\loss}_{f, D}(\bar{w}) - \loss_{f, D}(\bar{w})| \leq \tau$ simultaneously
except with probability $\epsilon/2$ (by a union bound). Thus, allowing for
failure probability $\epsilon$, we assume that we are in the case when the
neighbourhood always has a mutation that decreases the expected loss by $\alpha$
(whenever the expected loss of the current representation is at least
$\epsilon$) and that all empirical expected losses are $\tau$-close to the true
expected losses.

Now let $w$ be the representation at some generation such that $\loss_{f, D}(w)
\geq \epsilon$, let $w^\prime \in \Neigh(w, \epsilon)$ such that $\loss_{f,
D}(w^\prime) \leq \loss_{f, D}(w) - \alpha$. Then, it is the case that
$\hat{\loss}_{f, D}(w^\prime) \leq \hat{\loss}_{f, D}(w) - 3\alpha/5$ (when
$\tau = \alpha/5$). Hence, for tolerance function $t = 3 \alpha/5$, for the
selection rule using $\bnsel$, $w^\prime \in \Bene$. Consequently $\Bene \neq
\emptyset$. Hence, the representation at the next generation is chosen from
$\Bene$. Let $\tilde{w}$ be the chosen representation. It must be the case that
$\hat{\loss}_{f, D}(\tilde{w}) \leq \hat{\loss}_{f, D}(w) - t$. Thus, we have
$\loss_{f, D}(\tilde{w}) \leq \loss_{f, D}(w) - \alpha/5$. Hence, the expected
loss decreases at least by $\alpha/5$.

Note that at no point can the expected loss by greater than $4kG^2B^2$ for any
representation in $R$. Hence, in at most $20 KG^2B^2/\alpha$ generations,
evolution reaches a representation with expected loss at most $\epsilon$. Note
the only parameter introduced which has an inverse polynomial dependence on $n$
is $p$. This implies that $s$ only has polylogarithmic dependence on $n$. This
concludes the proof of the theorem.
\end{proof}

\begin{remark} We note that the same evolutionary mechanism works when evolving
the class $\lin^k_{0, u}$, as long as the sparsity $K$ of the representation
class is allowed polynomial dependence on $1/\epsilon$, the accuracy parameter.
This is consistent with the notion of attribute-efficiency, where the goal is
that the information complexity should be polylogarithmic in the number of
attributes, but may depend polynomially on $1/\epsilon$.
\end{remark}
