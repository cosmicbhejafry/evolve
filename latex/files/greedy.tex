In this section, we present a different evolutionary mechanism for evolving
sparse linear functions. This algorithm essentially is an adaptation of a greedy
algorithm commonly known as orthogonal matching pursuit (OMP) in the signal
processing literature (see ~\cite{Donoho:2006-recovery, Tropp:2004-greed}). We
require three assumptions to prove that this mechanism succeeds, which are
considerably stronger than the assumptions used earlier.
%%
%%
\begin{enumerate}
%
\item For any two variables, $x_i$ and $x_j$, the correlation coefficient,
$\corr(x_i, x_j) \leq \frac{1}{2k}$, where $k$ is the sparsity of the target,
where $k$ is the sparsity of the target linear function.
%
\item The selection rule is based on \emph{optimisation}, \ie nature selects the
(almost) best mutation from the candidate pool. 
%
\item Evolution is allowed \emph{initialisation}; the starting representation
corresponds to the $0$ vector.
%
\end{enumerate}

Nevertheless, the algorithm is appealing for its simplicity and also for the fact
that it is a \emph{proper} evolutionary mechanism -- it never uses a
representation that is not a $k$-sparse linear function. 

Let $C^k_{0, u}$ be the concept class from which the \emph{ideal function} is
chosen.\footnote{Here, we no longer need the fact that each coefficient in the
target linear function has magnitude at least $l$.} Let $R = \{w ~|~
\sparsity(w) \leq k, w_i \in [-B, B] \}$. Now, define the action of the mutator
as follows (we will define the parameters $B$, $\lambda$, and $m$ later
\vknote{FILL THIS}): For $w \in R$, 
%%
\begin{enumerate}
%%
\item (Adding) With probability $\lambda$ do the following:
Recall $\NZ(w)$ denotes the non-zero entries of $w$. If $|\NZ(w)| < k$, choose
$i \in [n] \setminus \NZ(w)$ uniformly at random. Let $w^\prime$ be such that
$w^\prime_j = w_i$ for $j \neq i$ and $w^\prime_i \in [-B, B]$ uniformly at
random. If $\NZ(w)= k$, let $w^\prime = w$. Then, the multiset $\Neigh(w,
\epsilon)$ is populated by $m$ independent draws from the procedure just
described.
%
\item With probability $1 - \lambda$ do the following:
%
\begin{enumerate}
\item (Scaling) With probability $1/2$, choose $\gamma \in [-1, 1]$ uniformly at
random and let $w^\prime = \gamma w$.
%
\item (Adjusting) With probability $1/2$, do the following.  Pick $i \in \NZ(w)$
uniformly at random.  Let $w^\prime$ be such that $w^\prime_j = w_j$ for $j \neq
i$, $w^\prime_i \in [-B, B]$ uniformly at random.
\end{enumerate}
Then, the multiset $\Neigh(w, \epsilon)$ is populated by $m$ independent draws
from the procedure just described.
\end{enumerate}

One thing to note in the above definition is that the mutations produced by the
mutator at any given time are correlated -- \ie they are all either of the kind that
add a new variable, or all of the kind that just manipulate existing variables.
At a high-level, we prove the success of this mechanism as follows:
\begin{enumerate}
\item Using mutations of type ``scaling'' or ``adjusting,'' a representation
that is close to the \emph{best} in the space is evolved.
\item When the representation is (close to) the best possible using current
variables, adding one of the variables that is present in the \emph{ideal
function}, but not in the current representation,
results in the greatest reduction of expected loss. Thus, selection
based on optimisation would always add a variable in $\NZ(f)$. By tuning
$\lambda$ appropriately, it is ensured that candidate mutations that add new
variables are never chosen until evolution has had time to approach the
\emph{best} representation using existing variables.
\end{enumerate}

To complete the proof we establish the following claims.

\begin{claim} \label{claim:date} If $\ltwonorm{f^S - w} \leq
\sqrt{\epsilon}{2k}$, then if $S \subsetneq \NZ(f)$, there exists $i \in \NZ(f)
\setminus S$ and $-B < a < b < B$, such that for any $\gamma \in [a, b]$,
$\loss_{f, D}(w + \gamma e^i) \leq \loss_{f, D}(w) - \epsilon/(4k^2)$ and for
any $j \not\in \NZ(f)$, $\beta \in [-B, B]$, $\loss_{f, D}(w + \beta e^j) \geq
\loss_{f, D}(w + \gamma e^i) + \epsilon/(4k^3)$. \end{claim}

\begin{claim} \label{claim:elderberry} If $\ltwonorm{f^S - w} \geq $\vknote{FILL
THIS}, then with probability at least \vknote{FILL THIS}, the mutator outputs a
mutation $w^\prime$ (of the type ``scaling'' or ``adjusting''), such that
$\loss_{f, D}(w^\prime) \leq \loss_{f, D}(w) - $\vknote{FILL THIS}.  \end{claim}

\begin{proof}[Proof of Claim~\ref{claim:date}] Since $S \subseteq \NZ(f)$, the
residual $r = f - w$ is such that $\NZ(r) \subseteq \NZ(f)$. Let $i \in \NZ(r)$
such that $|r_i||\ltwonorm{e^i}|$ is the maximum. Then, we have:
\begin{align}
\left|\frac{\ip{e^i}{r}}{\ltwonorm{e^i}} \right| &\geq
\frac{|r_i|\ip{e^i}{e^i}}{\ltwonorm{e^i}} - \sum_{j \in \NZ(r), j \neq i}
\frac{|r_j|\ip{e^i}{e^j}}{\ltwonorm{e^i}} \nonumber  \\
&\geq |r_i|\ltwonorm{e^i} - \frac{1}{2k} \sum_{j \in \NZ(r), j \neq i}
|r_j| \ltwonorm{e^j} \label{eqn:chives} \\
&\geq |r_i| \ltwonorm \cdot \frac{k+1}{2k}, \nonumber
\end{align}
where in the last two steps we used the fact that $\corr(x_i, x_j) =
\ip{e^i}{e^j}/(\ltwonorm{e^i} \ltwonorm{e^j}) \leq 1/(2k)$ and that $|\NZ(r)
\setminus \{ i \}| \leq k-1$. On the other hand, for any $i^\prime \not\in
\NZ(r)$, we have
\begin{align}
\left| \frac{\ip{e^{i^\prime}}{r}}{\ltwonorm{e^{i^\prime}}} \right| &\leq
\sum_{j \in \NZ(r)} \frac{|r_j\ip{e^{i^\prime}}{e^j}|}{\ltwonorm{e^{i^\prime}}}
\nonumber \\
&\leq \frac{1}{2k} \sum_{j \in \NZ(r)} |r_j| \ltwonorm{e^j} \leq \frac{1}{2}
|r_i| \ltwonorm{e^i} \label{eqn:dill}
\end{align}

Here, again in the last two steps, we've used the fact that $\corr(x_{i^\prime},
x_j) \leq 1/(2k)$ and the fact that $|r_i| \ltwonorm{e^i}$ is the largest such
term. 

First, we claim that if $\ltwonorm{r}^2 \geq \epsilon$, then the $i$ that
maximised $|r_i| \ltwonorm{e^i}$ must be from the set $\NZ(f) \setminus S$. Note
that $\ltwonorm{r}^2 = \ltwonorm{f - w}^2 = \loss_{f, D}(w)$, so if
$\ltwonorm{r}^2 \leq \epsilon$, evolution has reached it's goal. Now, we have by
triangle inequality, $\sum_{i \in \NZ(r)} |r_i| \ltwonorm{e^i} \geq \ltwonorm{r}
\geq \sqrt{\epsilon}$. Hence, it must be the case that $|r_i| \ltwonorm{e^i}
\geq \sqrt{\epsilon}{k}$. For contradiction, if $i \in S$, then since $f^S$ is
the projection of $f$ in the space spanned by $S$, we have $r = f - f^S + f^S -
w$ and $\ip{e^i}{r} = \ip{e^i}{f^S - w}$, since $\ip{e^i}{f - f^S} = 0$. But, by
assumption of claim, $|\ip{e^i}{r}|\leq \ltwonorm{e^i} \ltwonorm{f^S - w} \leq
\ltwonorm{e^i} \sqrt{\epsilon}{2k}$. But, by (\ref{eqn:chives}), we know that
$|\ip{e^i}{r}| \geq (k+1/(2k)) |r_i| \ltwonorm{e^i}^2 > \ltwonorm
\sqrt{\epsilon}/2k$. Thus, it cannot be the case that $i \in S$. 

Let $w^\prime = w + \gamma e^i$. Then, 
\begin{align} 
\ltwonorm{f - (w + \gamma e^i)}^2 - \ltwonorm{f - w}^2 &= - 2 \gamma \ip{f -
w}{e^i} + \gamma^2 \ltwonorm{e^i}^2 \nonumber
&\leq - \ltwonorm{e^i}^2 (|\gamma| |r_i| \frac{k+1}{k} - |\gamma|^2 \nonumber
\intertext{Now suppose $\gamma$ satisfies $1 - \delta \leq
(2|\gamma|k)/(|r_i|(k+1)) \leq 1$, then using the fact that the quadratic
function on the RHS is maximised at $|\gamma| = (k+1) |r_i|/(2k)$, we have}
\ltwonorm{f - (w + \gamma e^i)}^2 - \ltwonorm{f - w}^2 &\leq - \ltwonorm{e^i}^2
\frac{r_i^2}{4} \frac{(k+1)^2}{k^2} (1 - \delta^2) \label{eqn:eggplant}
\end{align}

Note that for any $i^\prime \not\in S$, the ``best'' representation of the form
$w + \beta e^{i^\prime}$ is when $\beta =
\ip{e^{i^\prime}}{r}/\ltwonorm{e^{i^\prime}}^2$, and the corresponding reduction
in squared loss is $(\ip{e^{i^\prime}}{r})^2/\ltwonorm{e^{i^\prime}}^2$. Thus,
for any $i^\prime \neq i$, we have
\begin{align}
\ltwonorm{f - (w + \beta e^{i^\prime})}^2 - \ltwonorm{f - w}^2 &\leq -
\frac{\ip{e^{i^\prime}}{r}^2}{\ltwonorm{e^{i^\prime}}^2} \nonumber
&\leq - \frac{r_i^2}{4} \ltwonorm{e^i}^2 &\mbox{Using~(\ref{eqn:dill})}
\nonumber
\end{align}
Now, by setting $\delta = \sqrt{1/k+1}$ completes the proof of the Claim.
\end{proof}

In the ensuing discussion, we will prove that for any target function
$w^* \in C^k_{l,u}$, if $w \in R$ is such that
$\lerr_D(w, w^*) = \ltwonorm{w - w^*}^2 > \epsilon$,
then the mutator outputs a mutation which decreases the error by a
non-negligible amount with non-negligible probability.
We will establish our claim by proving the following claims.

\begin{enumerate}
\item[Claim A] If $\ltwonorm{w} \ge 2 \ltwonorm{w^*_S}$, then for $M \ge 1$,
with probability at least $1/16 - \lambda/8 \ge $ \eanote{something} the mutator outputs
a mutation that reduces $\lerror$ by at least $\ltwonorm{w - w^*_S}^2 / 12$.
\item[Claim B] If $\ltwonorm{w} \le 2 \ltwonorm{w^*_S}$ and
$\ltwonorm{w - w^*_S} \ge \alpha$, then with probability at least $\alpha$,
there exists a mutation that decreases the $\lerror$ by at least $\alpha$.
\item[Claim C] If $\Delta^2 > 1 - 1/(2 K - 1)$ and
$\ltwonorm{w - w^*_S}^2 < \epsilon$, then $w^\prime$ will be
such that either $\sparseset(w^\prime) = \sparseset(w)$ and
$\ltwonorm{w^\prime - w^*_S}^2 < \epsilon$, or
$\sparseset(w^\prime) \setminus \sparseset(w) = \{i\}$ where $i \in \sparseset(w^*)$
and $w^\prime$ reduces $\lerror$ \eanote{by at least some amount}.
\end{enumerate}

To establish Claims A and B above, we follow the lines of reasoning from the
previous section.
For Claim A, we consider the case where selection has chosen the scaling class
of mutants.  With the assumption that $\ltwonorm{w} \ge 2 \ltwonorm{w_S^2}$ and
for each of the generated mutants $\gamma w$ with $\gamma \in [1/2, 3/4]$,
we obtain

\[
\ltwonorm{\gamma w - w^*_S}^2 \le \ltwonorm{w - w^*_S}^2 - \frac{1}{12}\ltwonorm{w - w^*_S}^2.
\]

For Claim B, \dots

For Claim C, in the case that selection chose the adding class of mutations,
we must show that the new index $i \in \sparseset(w^*)$, where
$\{i\} = \sparseset(w^\prime) \setminus \sparseset(w)$.
We show that among the set of mutants output by the mutator, with high probability
there will exist at least one mutant such that it reduces $\lerror$ by something.
\eanote{Explain how this relates to the proof in Donoho and give the correct citation(s)}.

Let $\hat{w}$ be the best approximation of $w^*$ such that $\hat{w}_i \in [-B, B]$
for some $i \in [n] \setminus \sparseset(w)$ and $\hat{w}_j = w_j$ for $j \neq i$;
in other words, $\hat{w}$ is the best possible output achieved by mutating $w$ with
one adding step.
In expectation, if $\lznorm{w} = k$, then $M / (n - k)$ of the generated mutants
$v$ will have $\sparseset(v) = \sparseset(\hat{w})$, and each will satisfy
$|v_i - \hat{w}_i| \le \delta$, $\delta < B$, and so
$\ltwonorm{v - \hat{w}} \le \delta$ with probability $\delta/2B$.
Thus in expectation, for $\delta > 2B(n-k) / M$, at least one mutant will be
``$\delta$-close'' to $\hat{w}$.
Below we argue that the remaining mutants with
$\sparseset(v) \neq \sparseset(\hat{w})$ will not be better than this mutant by
a margin of at least \eanote{fill in this condition}.

We must show that $i \in \sparseset(w^*)$ and furthermore that
\eanote{fill in this condition}.
Selection starts with the initial representation, $w=0$.
Without loss of generality, arrange $w^*$ so that $\sparseset(w^*) = [K]$,
in decreasing order of the values $\vert w^*_k \vert \sqrt{\var(X_k)}$.
Initially, the residual $w^* - w$ is $w^*$.
Let $w^*_{\{j\}}$ be the best approximation of $w^*$ with
$\sparseset(w^*_{\{j\}}) = \{j\}$, so $w^*_{\{j\}}$ \dots
With high probability, the mutator generates at least one mutant
$\hat{w}_{\{j\}}$ for each $j \in [n]$ that minimizes the residual to within
$\delta$ of the optimal solution restricted to $\{j\}$, i.e.,
$\ltwonorm{\hat{w}_{\{j\}} - w^*}^2 < \ltwonorm{w^*_{\{j\}} - w^*}^2 + \delta$.
The $\lerror$ of $w^*_{\{j\}}$ is
\eanote{Check the last step below!}

\begin{align*}
\ltwonorm{w^*_{\{j\}} - w^*}^2 = \min_{w_{\{j\}}}{\ltwonorm{w_{\{j\}} - w^*}}^2
&= \bigg \Vert e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2} - w^* \bigg \Vert^2 \\
&= \ltwonorm{w^*}^2 - 2 \bigg \langle e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2}, w^* \bigg \rangle + \bigg \Vert e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2} \bigg \Vert^2 \\
&= \ltwonorm{w^*}^2 - \frac{(\ip{e_j}{w^*})^2}{\ltwonorm{e_j}^2} \ge 0.
\end{align*}

\noindent Note that

\[
\frac{(\ip{e_j}{w^*})^2}{\ltwonorm{e_j}^2}
= \frac{(\E[(e_j \cdot X)(w^* \cdot X)])^2}{\E[(e_j \cdot X)(e_j \cdot X)]}
= \frac{(\E[X_j (w^* \cdot X)])^2}{\E[X_j^2]}
= \frac{(\sum_{k=1}^K w^*_k \E[X_j X_k]))^2}{\var(X_j)}
\]

For selection to choose a mutant $\hat{w}_{\{j\}}$ with
$j \in \sparseset(w^*) = [K]$, it must be the case that for any $i > K$,

\[
\bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
> \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert + \eanote{~something}.
\]

We construct lower and upper bounds for the left- and right-hand sides,
respectively, making use of the fact that
$|\corr(X_i, X_j)| \leq 1 - \Delta^2$ from Lemma~\ref{} and also
that we arranged the $\vert w^*_k \vert \sqrt{\var(X_k)}$ in decreasing order.
On the left,

\begin{align*}
\bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
&= \frac{\vert\sum_{k=1}^K w^*_k \E[X_1 X_k]\vert}{\sqrt{\var(X_1)}} \\
&\ge \frac{\vert w^*_1 \E[X_1 X_1]\vert}{\sqrt{\var(X_1)}} - \frac{\sum_{k=2}^K \vert w^*_k \E[X_1 X_k] \vert}{\sqrt{\var(X_1)}} \\
&= \vert w^*_1 \vert \sqrt{\var(X_1)} - \sum_{k=2}^K \vert w^*_k \vert \sqrt{\var(X_k)}~\vert \corr(X_1, X_k) \vert \\
&\ge \vert w^*_1 \vert \sqrt{\var(X_1)}(1 - (1 - \Delta^2) (K-1))
\end{align*}

\noindent On the right,

\begin{align*}
\bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert
= \frac{\vert\sum_{k=1}^K w^*_k \E[X_i X_k]\vert}{\sqrt{\var(X_i)}}
\le \sum_{k=1}^K \vert w^*_k \vert \frac{\vert \E[X_i X_k]\vert}{\sqrt{\var(X_i)}}
&= \sum_{k=1}^K \vert w^*_k \vert \sqrt{\var(X_k)} \corr(X_i, X_k) \\
&\le \vert w^*_1 \vert \sqrt{\var(X_1)} (1 - \Delta^2) K.
\end{align*}

\noindent Putting these bounds together now gives the requirement

\[
\bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
\ge \vert w^*_1 \vert \sqrt{\var(X_1)}(1 - (1 - \Delta^2) (K-1))
> \vert w^*_1 \vert \sqrt{\var(X_1)} (1 - \Delta^2) K
\ge \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert,
\]

\noindent and so

\begin{align*}
1 - (1 - \Delta^2) (K-1) = 2 - K + \Delta^2 K - \Delta^2 &> (1 - \Delta^2) K \\
%\Rightarrow & \qquad 1 + (1 - \Delta^2) > 2 (1 - \Delta^2) K
%\qquad \Rightarrow \qquad K < \frac{1}{2} \biggl( 1 + \frac{1}{1 - \Delta^2} \biggr)
\Rightarrow \qquad \Delta^2 &> 1 - \frac{1}{2 K - 1}
\end{align*}

\noindent  This is the condition for $\Delta^2$ in Claim C.
\eanote{Finally, argue that subsequent steps work too.}
\eanote{And then we should say something about the CS analog.}
