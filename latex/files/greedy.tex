We define a mutator that, under strong enough selection,
minimizes $\lerr_D(w, w^*)$ in a greedy fashion with respect to $\sparseset(w)$.
For $w \in R$, define the mutated representation $w^\prime$, output by the mutator as:
\begin{enumerate}
\item With probability $1/2$ do nothing, set $w^\prime = w$.
\item (Scaling) With probability $1/4 - \lambda/2$, choose $\gamma \in [-1, 1]$
uniformly at random and let $w^\prime = \gamma w$.
\item (Adjusting) With probability $1/4 - \lambda/2$, do the following.
Let $S_w = \{ i~|~ w_i \neq 0\}$. Pick $i \in S_w$ uniformly at random.
Let $w^\prime$ denote the mutated representation, where
$w^\prime_j = w_j$ for $j \neq i$, $w^\prime_i \in [-B, B]$ uniformly at random.
\item (Adding) With the remaining probability $\lambda$, do the following.
If $|S_w| < K$, choose $i \in [n] \setminus S_w$ uniformly at random.
Let $w^\prime$ be the mutated representation, where $w_j^\prime = w_j$ for
$j \neq i$, $w^\prime_i$ is chosen uniformly in $[-B, B]$.
Otherwise, if $|S_w| = K$, do nothing, set $w^\prime = w$.
\end{enumerate}

We present a selection framework that results in greedy progress in the sense
that, with high probability, $\lerr_D(w, w^*_S)$ is minimized for fixed $|S|$
before an adding step increments $|S|$.
Specifically, selection initializes $w = 0$ so that $\sparseset(w) = \emptyset$,
then greedily identifies $\sparseset(w^*)$ such that
$\sparseset(w) \subseteq \sparseset(w^\prime) \subseteq \sparseset(w^*)$
throughout the selection process.
This is inspired by and can be thought of as a robust version of a
greedy algorithm known in the signal processing literature as
orthogonal matching pursuit (OMP)~\cite{Tropp:2004-greed,Donoho:2006-recovery}.
For an excellent review of related algorithms and issues from the perspective
of compressed sensing, see~\cite{Donoho:2009-sparse}.

\begin{enumerate}
\item (Initialization) Selection initializes $w = 0$, so $\sparseset(w) = \emptyset$.
\item (Main iteration)
	\begin{enumerate}
	\item Choose one class of mutations at random according to their probabilities.
	\item Generate $M > 2B(n - K) / \delta$ \eanote{(+ extra?)} mutants uniformly at random within that class.
	\item Set $w^\prime$ to the winner.
	\end{enumerate}
\end{enumerate}

In the ensuing discussion, we will prove that for any target function
$w^* \in C^k_{l,u}$, if $w \in R$ is such that
$\lerr_D(w, w^*) = \ltwonorm{w - w^*}^2 > \epsilon$,
then the mutator outputs a mutation which decreases the error by a
non-negligible amount with non-negligible probability.
We will establish our claim by proving the following claims.

\begin{enumerate}
\item[Claim A] If $\ltwonorm{w} \ge 2 \ltwonorm{w^*_S}$, then for $M \ge 1$,
with probability at least $1/16 - \lambda/8 \ge $ \eanote{something} the mutator outputs
a mutation that reduces $\lerror$ by at least $\ltwonorm{w - w^*_S}^2 / 8$.
\eanote{I think this 1/8 is correct and applies in the above section.}
\item[Claim B] If $\ltwonorm{w} \le 2 \ltwonorm{w^*_S}$ and
$\ltwonorm{w - w^*_S} \ge \alpha$, then with probability at least $\alpha$,
there exists a mutation that decreases the $\lerror$ by at least $\alpha$.
\item[Claim C] If $\Delta^2 > 1 - 1/(2 K - 1)$ and
$\ltwonorm{w - w^*_S}^2 < \epsilon$, then $w^\prime$ will be
such that either $\sparseset(w^\prime) = \sparseset(w)$ and
$\ltwonorm{w^\prime - w^*_S}^2 < \epsilon$, or
$\sparseset(w^\prime) \setminus \sparseset(w) = \{i\}$ where $i \in \sparseset(w^*)$
and $w^\prime$ reduces $\lerror$ \eanote{by at least some amount}.
\end{enumerate}

To establish Claims A and B above, we follow the lines of reasoning from the
previous section.
For Claim A, we consider the case where selection has chosen the scaling class
of mutants.  With the assumption that $\ltwonorm{w} \ge 2 \ltwonorm{w_S^2}$ and
for each of the generated mutants $\gamma w$ with $\gamma \in [1/2, 3/4]$,
we obtain

\[
\ltwonorm{\gamma w - w^*_S}^2 \le \ltwonorm{w - w^*_S}^2 - \frac{1}{8}\ltwonorm{w - w^*_S}^2.
\]

For Claim B, \dots

For Claim C, in the case that selection chose the adding class of mutations,
we must show that the new index $i \in \sparseset(w^*)$, where
$\{i\} = \sparseset(w^\prime) \setminus \sparseset(w)$.
We show that among the set of mutants output by the mutator, with high probability
there will exist at least one mutant such that it reduces $\lerror$ by something.
\eanote{Explain how this relates to the proof in Donoho and give the correct citation(s)}.

Let $\hat{w}$ be the best approximation of $w^*$ such that $\hat{w}_i \in [-B, B]$
for some $i \in [n] \setminus \sparseset(w)$ and $\hat{w}_j = w_j$ for $j \neq i$;
in other words, $\hat{w}$ is the best possible output achieved by mutating $w$ with
one adding step.
In expectation, if $\lznorm{w} = k$, then $M / (n - k)$ of the generated mutants
$v$ will have $\sparseset(v) = \sparseset(\hat{w})$, and each will satisfy
$|v_i - \hat{w}_i| \le \delta$, $\delta < B$, and so
$\ltwonorm{v - \hat{w}} \le \delta$ with probability $\delta/2B$.
Thus in expectation, for $\delta > 2B(n-k) / M$, at least one mutant will be
``$\delta$-close'' to $\hat{w}$.
Below we argue that the remaining mutants with
$\sparseset(v) \neq \sparseset(\hat{w})$ will not be better than this mutant by
a margin of at least \eanote{fill in this condition}.

We must show that $i \in \sparseset(w^*)$ and furthermore that
\eanote{fill in this condition}.
Selection starts with the initial representation, $w=0$.
Without loss of generality, arrange $w^*$ so that $\sparseset(w^*) = [K]$,
in decreasing order of the values $\vert w^*_k \vert \sqrt{\var(X_k)}$.
Initially, the residual $w^* - w$ is $w^*$.
Let $w^*_{\{j\}}$ be the best approximation of $w^*$ with
$\sparseset(w^*_{\{j\}}) = \{j\}$, so $w^*_{\{j\}}$ \dots
With high probability, the mutator generates at least one mutant
$\hat{w}_{\{j\}}$ for each $j \in [n]$ that minimizes the residual to within
$\delta$ of the optimal solution restricted to $\{j\}$, i.e.,
$\ltwonorm{\hat{w}_{\{j\}} - w^*}^2 < \ltwonorm{w^*_{\{j\}} - w^*}^2 + \delta$.
The $\lerror$ of $w^*_{\{j\}}$ is
\eanote{Check the last step below!}

\begin{align*}
\ltwonorm{w^*_{\{j\}} - w^*}^2 = \min_{w_{\{j\}}}{\ltwonorm{w_{\{j\}} - w^*}}^2
&= \bigg \Vert e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2} - w^* \bigg \Vert^2 \\
&= \ltwonorm{w^*}^2 - 2 \bigg \langle e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2}, w^* \bigg \rangle + \bigg \Vert e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2} \bigg \Vert^2 \\
&= \ltwonorm{w^*}^2 - \frac{(\ip{e_j}{w^*})^2}{\ltwonorm{e_j}^2} \ge 0.
\end{align*}

\noindent Note that

\[
\frac{(\ip{e_j}{w^*})^2}{\ltwonorm{e_j}^2}
= \frac{(\E[(e_j \cdot X)(w^* \cdot X)])^2}{\E[(e_j \cdot X)(e_j \cdot X)]}
= \frac{(\E[X_j (w^* \cdot X)])^2}{\E[X_j^2]}
= \frac{(\sum_{k=1}^K w^*_k \E[X_j X_k]))^2}{\var(X_j)}
\]

For selection to choose a mutant $\hat{w}_{\{j\}}$ with
$j \in \sparseset(w^*) = [K]$, it must be the case that for any $i > K$,

\[
\bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
> \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert + \eanote{~something}.
\]

We construct lower and upper bounds for the left- and right-hand sides,
respectively, making use of the fact that
$|\corr(X_i, X_j)| \leq 1 - \Delta^2$ from Lemma~\ref{} and also
that we arranged the $w^*_k \vert \sqrt{\var(X_k)}$ in decreasing order.
On the left,

\begin{align*}
\bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
&= \frac{\vert\sum_{k=1}^K w^*_k \E[X_1 X_k]\vert}{\sqrt{\var(X_1)}} \\
&\ge \frac{\vert w^*_1 \E[X_1 X_1]\vert}{\sqrt{\var(X_1)}} - \frac{\sum_{k=2}^K \vert w^*_k \E[X_1 X_k] \vert}{\sqrt{\var(X_1)}} \\
&= \vert w^*_1 \vert \sqrt{\var(X_1)} - \sum_{k=2}^K \vert w^*_k \vert \sqrt{\var(X_k)}~\vert \corr(X_1, X_k) \vert \\
&\ge \vert w^*_1 \vert \sqrt{\var(X_1)}(1 - (1 - \Delta^2) (K-1))
\end{align*}

\noindent On the right,

\begin{align*}
\bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert
= \frac{\vert\sum_{k=1}^K w^*_k \E[X_i X_k]\vert}{\sqrt{\var(X_i)}}
\le \sum_{k=1}^K \vert w^*_k \vert \frac{\vert \E[X_i X_k]\vert}{\sqrt{\var(X_i)}}
&= \sum_{k=1}^K \vert w^*_k \vert \sqrt{\var(X_k)} \corr(X_i, X_k) \\
&\le \vert w^*_1 \vert \sqrt{\var(X_1)} (1 - \Delta^2) K.
\end{align*}

\noindent Putting these bounds together now gives the requirement

\[
\bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
\ge \vert w^*_1 \vert \sqrt{\var(X_1)}(1 - (1 - \Delta^2) (K-1))
> \vert w^*_1 \vert \sqrt{\var(X_1)} (1 - \Delta^2) K
\ge \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert,
\]

\noindent and so

\begin{align*}
1 - (1 - \Delta^2) (K-1) = 2 - K + \Delta^2 K - \Delta^2 &> (1 - \Delta^2) K \\
%\Rightarrow & \qquad 1 + (1 - \Delta^2) > 2 (1 - \Delta^2) K
%\qquad \Rightarrow \qquad K < \frac{1}{2} \biggl( 1 + \frac{1}{1 - \Delta^2} \biggr)
\Rightarrow \qquad \Delta^2 &> 1 - \frac{1}{2 K - 1}
\end{align*}

\noindent  This is the condition for $\Delta^2$ in Claim C.
\eanote{Finally, argue that subsequent steps work too.}
\eanote{And then we should say something about the CS analog.}