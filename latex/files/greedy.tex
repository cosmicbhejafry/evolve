In this section, we present a different evolutionary mechanism for evolving
sparse linear functions. This algorithm essentially is an adaptation of a greedy
algorithm commonly known as orthogonal matching pursuit (OMP) in the signal
processing literature (see ~\cite{Donoho:2006-recovery, Tropp:2004-greed}). We
require three assumptions to prove that this mechanism succeeds, which are
considerably stronger than the assumptions used earlier.
%%
%%
\begin{enumerate}
%
\item For any two variables, $x_i$ and $x_j$, the correlation coefficient,
$\corr(x_i, x_j) \leq \frac{1}{2k}$, where $k$ is the sparsity of the target,
where $k$ is the sparsity of the target linear function.
%
\item The selection rule is based on \emph{optimisation}, \ie nature selects the
(almost) best mutation from the candidate pool. 
%
\item Evolution is allowed \emph{initialisation}; the starting representation
corresponds to the $0$ vector.
%
\end{enumerate}

Nevertheless, the algorithm is appealing for its simplicity and also for the fact
that it is a \emph{proper} evolutionary mechanism -- it never uses a
representation that is not a $k$-sparse linear function. 

Let $C^k_{0, u}$ be the concept class from which the \emph{ideal function} is
chosen.\footnote{Here, we no longer need the fact that each coefficient in the
target linear function has magnitude at least $l$.} Let $R = \{w ~|~
\sparsity(w) \leq k, w_i \in [-B, B] \}$. Now, define the action of the mutator
as follows (we will define the parameters $B$, $\lambda$, and $m$ later
\vknote{FILL THIS}): For $w \in R$, 
%%
\begin{enumerate}
%%
\item (Adding) With probability $\lambda$ do the following:
Recall $\NZ(w)$ denotes the non-zero entries of $w$. If $|\NZ(w)| < k$, choose
$i \in [n] \setminus \NZ(w)$ uniformly at random. Let $w^\prime$ be such that
$w^\prime_j = w_i$ for $j \neq i$ and $w^\prime_i \in [-B, B]$ uniformly at
random. If $\NZ(w)= k$, let $w^\prime = w$. Then, the multiset $\Neigh(w,
\epsilon)$ is populated by $m$ independent draws from the procedure just
described.
%
\item With probability $1 - \lambda$ do the following:
%
\begin{enumerate}
\item (Scaling) With probability $1/2$, choose $\gamma \in [-1, 1]$ uniformly at
random and let $w^\prime = \gamma w$.
%
\item (Adjusting) With probability $1/2$, do the following.  Pick $i \in \NZ(w)$
uniformly at random.  Let $w^\prime$ be such that $w^\prime_j = w_j$ for $j \neq
i$, $w^\prime_i \in [-B, B]$ uniformly at random.
\end{enumerate}
Then, the multiset $\Neigh(w, \epsilon)$ is populated by $m$ independent draws
from the procedure just described.
\end{enumerate}

One thing to note in the above definition is that the mutations produced by the
mutator at any given time are correlated -- \ie they are all either of the kind that
add a new variable, or all of the kind that just manipulate existing variables.
At a high-level, we prove the success of this mechanism as follows:
\begin{enumerate}
\item Using mutations of type ``scaling'' or ``adjusting,'' a representation
that is close to the \emph{best} in the space is evolved.
\item When the representation is (close to) the best possible using current
variables, adding one of the variables that is present in the \emph{ideal
function}, but not in the current representation,
results in the greatest reduction of expected loss. Thus, selection
based on optimisation would always add a variable in $\NZ(f)$. By tuning
$\lambda$ appropriately, it is ensured that candidate mutations that add new
variables are never chosen until evolution has had time to approach the
\emph{best} representation using existing variables.
\end{enumerate}

To complete the proof we establish the following claims.

\begin{claim} \label{claim:date} If $\ltwonorm{f^S - w} \leq
\sqrt{\epsilon}/{2k}$, then if $S \subsetneq \NZ(f)$, there exists $i \in \NZ(f)
\setminus S$ and $-B < a < b < B$, such that for any $\gamma \in [a, b]$,
$\loss_{f, D}(w + \gamma e^i) \leq \loss_{f, D}(w) - \epsilon/(4k^2)$ and for
any $j \not\in \NZ(f)$, $\beta \in [-B, B]$, $\loss_{f, D}(w + \beta e^j) \geq
\loss_{f, D}(w + \gamma e^i) + \epsilon/(4k^3)$. \end{claim}

\begin{claim} \label{claim:elderberry} With probability at least $(1 - \lambda)
\min\{1/8, \ltwonorm{f^S - w}/(4k^2)\}$, there exists a mutation that reduces the
squared loss by at least $\ltwonorm{f^S - w}^2/12$. \end{claim}

\begin{proof}[Proof of Claim~\ref{claim:date}] Since $S \subseteq \NZ(f)$, the
residual $r = f - w$ is such that $\NZ(r) \subseteq \NZ(f)$. Let $i \in \NZ(r)$
such that $|r_i||\ltwonorm{e^i}|$ is the maximum. Then, we have:
\begin{align}
\left|\frac{\ip{e^i}{r}}{\ltwonorm{e^i}} \right| &\geq
\frac{|r_i|\ip{e^i}{e^i}}{\ltwonorm{e^i}} - \sum_{j \in \NZ(r), j \neq i}
\frac{|r_j|\ip{e^i}{e^j}}{\ltwonorm{e^i}} \nonumber  \\
&\geq |r_i|\ltwonorm{e^i} - \frac{1}{2k} \sum_{j \in \NZ(r), j \neq i}
|r_j| \ltwonorm{e^j} \label{eqn:chives} \\
&\geq |r_i| \ltwonorm \cdot \frac{k+1}{2k}, \nonumber
\end{align}
where in the last two steps we used the fact that $\corr(x_i, x_j) =
\ip{e^i}{e^j}/(\ltwonorm{e^i} \ltwonorm{e^j}) \leq 1/(2k)$ and that $|\NZ(r)
\setminus \{ i \}| \leq k-1$. On the other hand, for any $i^\prime \not\in
\NZ(r)$, we have
\begin{align}
\left| \frac{\ip{e^{i^\prime}}{r}}{\ltwonorm{e^{i^\prime}}} \right| &\leq
\sum_{j \in \NZ(r)} \frac{|r_j\ip{e^{i^\prime}}{e^j}|}{\ltwonorm{e^{i^\prime}}}
\nonumber \\
&\leq \frac{1}{2k} \sum_{j \in \NZ(r)} |r_j| \ltwonorm{e^j} \leq \frac{1}{2}
|r_i| \ltwonorm{e^i} \label{eqn:dill}
\end{align}

Here, again in the last two steps, we've used the fact that $\corr(x_{i^\prime},
x_j) \leq 1/(2k)$ and the fact that $|r_i| \ltwonorm{e^i}$ is the largest such
term. 

First, we claim that if $\ltwonorm{r}^2 \geq \epsilon$, then the $i$ that
maximised $|r_i| \ltwonorm{e^i}$ must be from the set $\NZ(f) \setminus S$. Note
that $\ltwonorm{r}^2 = \ltwonorm{f - w}^2 = \loss_{f, D}(w)$, so if
$\ltwonorm{r}^2 \leq \epsilon$, evolution has reached it's goal. Now, we have by
triangle inequality, $\sum_{i \in \NZ(r)} |r_i| \ltwonorm{e^i} \geq \ltwonorm{r}
\geq \sqrt{\epsilon}$. Hence, it must be the case that $|r_i| \ltwonorm{e^i}
\geq \sqrt{\epsilon}{k}$. For contradiction, if $i \in S$, then since $f^S$ is
the projection of $f$ in the space spanned by $S$, we have $r = f - f^S + f^S -
w$ and $\ip{e^i}{r} = \ip{e^i}{f^S - w}$, since $\ip{e^i}{f - f^S} = 0$. But, by
assumption of claim, $|\ip{e^i}{r}|\leq \ltwonorm{e^i} \ltwonorm{f^S - w} \leq
\ltwonorm{e^i} \sqrt{\epsilon}{2k}$. But, by (\ref{eqn:chives}), we know that
$|\ip{e^i}{r}| \geq (k+1/(2k)) |r_i| \ltwonorm{e^i}^2 > \ltwonorm
\sqrt{\epsilon}/2k$. Thus, it cannot be the case that $i \in S$. 

Let $w^\prime = w + \gamma e^i$. Then, 
\begin{align} 
\ltwonorm{f - (w + \gamma e^i)}^2 - \ltwonorm{f - w}^2 &= - 2 \gamma \ip{f -
w}{e^i} + \gamma^2 \ltwonorm{e^i}^2 \nonumber
&\leq - \ltwonorm{e^i}^2 (|\gamma| |r_i| \frac{k+1}{k} - |\gamma|^2 \nonumber
\intertext{Now suppose $\gamma$ satisfies $1 - \delta \leq
(2|\gamma|k)/(|r_i|(k+1)) \leq 1$, then using the fact that the quadratic
function on the RHS is maximised at $|\gamma| = (k+1) |r_i|/(2k)$, we have}
\ltwonorm{f - (w + \gamma e^i)}^2 - \ltwonorm{f - w}^2 &\leq - \ltwonorm{e^i}^2
\frac{r_i^2}{4} \frac{(k+1)^2}{k^2} (1 - \delta^2) \label{eqn:eggplant}
\end{align}

Note that for any $i^\prime \not\in S$, the ``best'' representation of the form
$w + \beta e^{i^\prime}$ is when $\beta =
\ip{e^{i^\prime}}{r}/\ltwonorm{e^{i^\prime}}^2$, and the corresponding reduction
in squared loss is $(\ip{e^{i^\prime}}{r})^2/\ltwonorm{e^{i^\prime}}^2$. Thus,
for any $i^\prime \neq i$, we have
\begin{align}
\ltwonorm{f - (w + \beta e^{i^\prime})}^2 - \ltwonorm{f - w}^2 &\leq -
\frac{\ip{e^{i^\prime}}{r}^2}{\ltwonorm{e^{i^\prime}}^2} \nonumber
&\leq - \frac{r_i^2}{4} \ltwonorm{e^i}^2 &\mbox{Using~(\ref{eqn:dill})}
\nonumber
\end{align}
Now, by setting $\delta = \sqrt{1/k+1}$ completes the proof of the Claim.
\end{proof}

\begin{proof}[Proof of Claim~\ref{claim:elderberry}] With probability $\lambda$,
the mutator only outputs mutations that add an extra variable. We ignore these
kinds of mutations in this analysis. However, with the remaining $1 - \lambda$
probability, the mutator outputs a mutation that is either of the type
``scaling'' or ``adjusting''. The proof follows along the lines of the proofs of
Claims~\ref{claim:apple} and \ref{claim:banana}. First, suppose that
$\ltwonorm{w} \geq 2 \ltwonorm{f^S}$. In this case, we claim that for $\gamma
\in [1/2, 3/4]$, the mutation $\gamma w$ reduces the squared loss by at least
$\ltwonorm{f^S - w}^2$. This analysis is completely identical to that in
Claim~\ref{claim:apple} and hence is omitted. The only difference is that the
probability that such a mutation is selected is $((1 - \lambda)/2) \cdot 1/4$. 

Next, we assume that $\ltwonorm{w} \leq 2 \ltwonorm{f^S}$. Let $r^S = f^S - w$.
Now, as in the proof of Claim~\ref{claim:date}, let $i \in \NZ(r^S) (= S)$ be
such that $|r^S_i| \ltwonorm{e^i}$ is maximised. Hence, the following is true:
\begin{align*}
\left| \frac{\ip{e^i}{r^S}}{\ltwonorm{e^i}} \right| &\geq |r^S_i|
\frac{\ip{e^i}{e^i}}{\ltwonorm{e^i}} - \sum_{j \in S, j \neq i} |r^S_j|
\frac{\ip{e^i}{e^j}}{\ltwonorm{e^i}} \\
&\geq |r_i| \ltwonorm{e^i} - \frac{1}{2k} \sum_{j \in S, j \neq i} |r_j| \ltwonorm{e^j}
&\mbox{Since $\corr(x_i, x_j) \leq \frac{1}{2k}$} \\
&\geq |r_i| \ltwonorm{e^i} - \frac{k-1}{2k} \cdot |r_i| \ltwonorm{e^i} &
\mbox{Since $|r_i| \ltwonorm{e^i}$ is the largest}
&\geq \frac{1}{2} |r_i| \ltwonorm{e^i} 
\end{align*}

Also, by triangle inequality we know that $\sum_{j \in S} |r_j| \ltwonorm{e^j}
\geq \ltwonorm{r^S}$; and hence by definition of $i$, $|r_i| \ltwonorm{e^i} \geq
\ltwonorm{r^S}/k$. Now, finally let $\beta = \ip{e^i}{r^S}/\ltwonorm{e^i}^2$,
and for $\gamma \in [\beta - |\beta|/2, \beta + |\beta|/2]$, consider the
mutation $w + \gamma e^i$. We have,
\begin{align*}
\ltwonorm{f^S - (w + \gamma e^i)}^2 - \ltwonorm{f^S - w}^2 &= -2 \gamma
\ip{e^i}{r^S} + \gamma^2 \ltwonorm{e^i}^2 \\
&= - \ltwonorm{e^i}^2(2 |\gamma| |\beta| - |\gamma|^2)
&\leq - \frac{ 3 \ltwonorm{e^i}^2 \beta^2}{4} &\mbox{For $\gamma \in [\beta -
|\beta|/2, \beta + |\beta|/2]$}
&\leq - \frac{3}{4} \ltwonorm{r^S}^2
\end{align*}

Finally, note that the probability of choosing such a mutation is at least $(1 -
\lambda)|\beta|/(4Bk)$ ($1 - \lambda$ for not choosing to add a new variable,
$1/2$ for choosing a mutation of type ``adjusting'', $1/k$ for choosing the
appropriate variable to adjust and $|\beta|/(2B)$ for choosing the correct
value). Combining the claims for mutations of the ``scaling'' and ``adjusting''
type proves the statement of the claim.
\end{proof}

%% VK %% In the ensuing discussion, we will prove that for any target function
%% VK %% $w^* \in C^k_{l,u}$, if $w \in R$ is such that
%% VK %% $\lerr_D(w, w^*) = \ltwonorm{w - w^*}^2 > \epsilon$,
%% VK %% then the mutator outputs a mutation which decreases the error by a
%% VK %% non-negligible amount with non-negligible probability.
%% VK %% We will establish our claim by proving the following claims.
%% VK %% 
%% VK %% \begin{enumerate}
%% VK %% \item[Claim A] If $\ltwonorm{w} \ge 2 \ltwonorm{w^*_S}$, then for $M \ge 1$,
%% VK %% with probability at least $1/16 - \lambda/8 \ge $ \eanote{something} the mutator outputs
%% VK %% a mutation that reduces $\lerror$ by at least $\ltwonorm{w - w^*_S}^2 / 12$.
%% VK %% \item[Claim B] If $\ltwonorm{w} \le 2 \ltwonorm{w^*_S}$ and
%% VK %% $\ltwonorm{w - w^*_S} \ge \alpha$, then with probability at least $\alpha$,
%% VK %% there exists a mutation that decreases the $\lerror$ by at least $\alpha$.
%% VK %% \item[Claim C] If $\Delta^2 > 1 - 1/(2 K - 1)$ and
%% VK %% $\ltwonorm{w - w^*_S}^2 < \epsilon$, then $w^\prime$ will be
%% VK %% such that either $\sparseset(w^\prime) = \sparseset(w)$ and
%% VK %% $\ltwonorm{w^\prime - w^*_S}^2 < \epsilon$, or
%% VK %% $\sparseset(w^\prime) \setminus \sparseset(w) = \{i\}$ where $i \in \sparseset(w^*)$
%% VK %% and $w^\prime$ reduces $\lerror$ \eanote{by at least some amount}.
%% VK %% \end{enumerate}
%% VK %% 
%% VK %% To establish Claims A and B above, we follow the lines of reasoning from the
%% VK %% previous section.
%% VK %% For Claim A, we consider the case where selection has chosen the scaling class
%% VK %% of mutants.  With the assumption that $\ltwonorm{w} \ge 2 \ltwonorm{w_S^2}$ and
%% VK %% for each of the generated mutants $\gamma w$ with $\gamma \in [1/2, 3/4]$,
%% VK %% we obtain
%% VK %% 
%% VK %% \[
%% VK %% \ltwonorm{\gamma w - w^*_S}^2 \le \ltwonorm{w - w^*_S}^2 - \frac{1}{12}\ltwonorm{w - w^*_S}^2.
%% VK %% \]
%% VK %% 
%% VK %% For Claim B, \dots
%% VK %% 
%% VK %% For Claim C, in the case that selection chose the adding class of mutations,
%% VK %% we must show that the new index $i \in \sparseset(w^*)$, where
%% VK %% $\{i\} = \sparseset(w^\prime) \setminus \sparseset(w)$.
%% VK %% We show that among the set of mutants output by the mutator, with high probability
%% VK %% there will exist at least one mutant such that it reduces $\lerror$ by something.
%% VK %% \eanote{Explain how this relates to the proof in Donoho and give the correct citation(s)}.
%% VK %% 
%% VK %% Let $\hat{w}$ be the best approximation of $w^*$ such that $\hat{w}_i \in [-B, B]$
%% VK %% for some $i \in [n] \setminus \sparseset(w)$ and $\hat{w}_j = w_j$ for $j \neq i$;
%% VK %% in other words, $\hat{w}$ is the best possible output achieved by mutating $w$ with
%% VK %% one adding step.
%% VK %% In expectation, if $\lznorm{w} = k$, then $M / (n - k)$ of the generated mutants
%% VK %% $v$ will have $\sparseset(v) = \sparseset(\hat{w})$, and each will satisfy
%% VK %% $|v_i - \hat{w}_i| \le \delta$, $\delta < B$, and so
%% VK %% $\ltwonorm{v - \hat{w}} \le \delta$ with probability $\delta/2B$.
%% VK %% Thus in expectation, for $\delta > 2B(n-k) / M$, at least one mutant will be
%% VK %% ``$\delta$-close'' to $\hat{w}$.
%% VK %% Below we argue that the remaining mutants with
%% VK %% $\sparseset(v) \neq \sparseset(\hat{w})$ will not be better than this mutant by
%% VK %% a margin of at least \eanote{fill in this condition}.
%% VK %% 
%% VK %% We must show that $i \in \sparseset(w^*)$ and furthermore that
%% VK %% \eanote{fill in this condition}.
%% VK %% Selection starts with the initial representation, $w=0$.
%% VK %% Without loss of generality, arrange $w^*$ so that $\sparseset(w^*) = [K]$,
%% VK %% in decreasing order of the values $\vert w^*_k \vert \sqrt{\var(X_k)}$.
%% VK %% Initially, the residual $w^* - w$ is $w^*$.
%% VK %% Let $w^*_{\{j\}}$ be the best approximation of $w^*$ with
%% VK %% $\sparseset(w^*_{\{j\}}) = \{j\}$, so $w^*_{\{j\}}$ \dots
%% VK %% With high probability, the mutator generates at least one mutant
%% VK %% $\hat{w}_{\{j\}}$ for each $j \in [n]$ that minimizes the residual to within
%% VK %% $\delta$ of the optimal solution restricted to $\{j\}$, i.e.,
%% VK %% $\ltwonorm{\hat{w}_{\{j\}} - w^*}^2 < \ltwonorm{w^*_{\{j\}} - w^*}^2 + \delta$.
%% VK %% The $\lerror$ of $w^*_{\{j\}}$ is
%% VK %% \eanote{Check the last step below!}
%% VK %% 
%% VK %% \begin{align*}
%% VK %% \ltwonorm{w^*_{\{j\}} - w^*}^2 = \min_{w_{\{j\}}}{\ltwonorm{w_{\{j\}} - w^*}}^2
%% VK %% &= \bigg \Vert e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2} - w^* \bigg \Vert^2 \\
%% VK %% &= \ltwonorm{w^*}^2 - 2 \bigg \langle e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2}, w^* \bigg \rangle + \bigg \Vert e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2} \bigg \Vert^2 \\
%% VK %% &= \ltwonorm{w^*}^2 - \frac{(\ip{e_j}{w^*})^2}{\ltwonorm{e_j}^2} \ge 0.
%% VK %% \end{align*}
%% VK %% 
%% VK %% \noindent Note that
%% VK %% 
%% VK %% \[
%% VK %% \frac{(\ip{e_j}{w^*})^2}{\ltwonorm{e_j}^2}
%% VK %% = \frac{(\E[(e_j \cdot X)(w^* \cdot X)])^2}{\E[(e_j \cdot X)(e_j \cdot X)]}
%% VK %% = \frac{(\E[X_j (w^* \cdot X)])^2}{\E[X_j^2]}
%% VK %% = \frac{(\sum_{k=1}^K w^*_k \E[X_j X_k]))^2}{\var(X_j)}
%% VK %% \]
%% VK %% 
%% VK %% For selection to choose a mutant $\hat{w}_{\{j\}}$ with
%% VK %% $j \in \sparseset(w^*) = [K]$, it must be the case that for any $i > K$,
%% VK %% 
%% VK %% \[
%% VK %% \bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
%% VK %% > \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert + \eanote{~something}.
%% VK %% \]
%% VK %% 
%% VK %% We construct lower and upper bounds for the left- and right-hand sides,
%% VK %% respectively, making use of the fact that
%% VK %% $|\corr(X_i, X_j)| \leq 1 - \Delta^2$ from Lemma~\ref{} and also
%% VK %% that we arranged the $\vert w^*_k \vert \sqrt{\var(X_k)}$ in decreasing order.
%% VK %% On the left,
%% VK %% 
%% VK %% \begin{align*}
%% VK %% \bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
%% VK %% &= \frac{\vert\sum_{k=1}^K w^*_k \E[X_1 X_k]\vert}{\sqrt{\var(X_1)}} \\
%% VK %% &\ge \frac{\vert w^*_1 \E[X_1 X_1]\vert}{\sqrt{\var(X_1)}} - \frac{\sum_{k=2}^K \vert w^*_k \E[X_1 X_k] \vert}{\sqrt{\var(X_1)}} \\
%% VK %% &= \vert w^*_1 \vert \sqrt{\var(X_1)} - \sum_{k=2}^K \vert w^*_k \vert \sqrt{\var(X_k)}~\vert \corr(X_1, X_k) \vert \\
%% VK %% &\ge \vert w^*_1 \vert \sqrt{\var(X_1)}(1 - (1 - \Delta^2) (K-1))
%% VK %% \end{align*}
%% VK %% 
%% VK %% \noindent On the right,
%% VK %% 
%% VK %% \begin{align*}
%% VK %% \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert
%% VK %% = \frac{\vert\sum_{k=1}^K w^*_k \E[X_i X_k]\vert}{\sqrt{\var(X_i)}}
%% VK %% \le \sum_{k=1}^K \vert w^*_k \vert \frac{\vert \E[X_i X_k]\vert}{\sqrt{\var(X_i)}}
%% VK %% &= \sum_{k=1}^K \vert w^*_k \vert \sqrt{\var(X_k)} \corr(X_i, X_k) \\
%% VK %% &\le \vert w^*_1 \vert \sqrt{\var(X_1)} (1 - \Delta^2) K.
%% VK %% \end{align*}
%% VK %% 
%% VK %% \noindent Putting these bounds together now gives the requirement
%% VK %% 
%% VK %% \[
%% VK %% \bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
%% VK %% \ge \vert w^*_1 \vert \sqrt{\var(X_1)}(1 - (1 - \Delta^2) (K-1))
%% VK %% > \vert w^*_1 \vert \sqrt{\var(X_1)} (1 - \Delta^2) K
%% VK %% \ge \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert,
%% VK %% \]
%% VK %% 
%% VK %% \noindent and so
%% VK %% 
%% VK %% \begin{align*}
%% VK %% 1 - (1 - \Delta^2) (K-1) = 2 - K + \Delta^2 K - \Delta^2 &> (1 - \Delta^2) K \\
%% VK %% %\Rightarrow & \qquad 1 + (1 - \Delta^2) > 2 (1 - \Delta^2) K
%% VK %% %\qquad \Rightarrow \qquad K < \frac{1}{2} \biggl( 1 + \frac{1}{1 - \Delta^2} \biggr)
%% VK %% \Rightarrow \qquad \Delta^2 &> 1 - \frac{1}{2 K - 1}
%% VK %% \end{align*}
%% VK %% 
%% VK %% \noindent  This is the condition for $\Delta^2$ in Claim C.
%% VK %% \eanote{Finally, argue that subsequent steps work too.}
%% VK %% \eanote{And then we should say something about the CS analog.}
