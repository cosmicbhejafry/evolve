In this section, we present a different evolutionary mechanism for evolving
sparse linear functions. This algorithm essentially is an adaptation of a greedy
algorithm commonly known as orthogonal matching pursuit (OMP) in the signal
processing literature (see ~\cite{Donoho:2006-recovery, Tropp:2004-greed}). We
require three assumptions to prove that this mechanism succeeds, which are
considerably stronger than the assumptions used earlier.
%%
%%
\begin{enumerate}
%
\item For any two variables, $x_i$ and $x_j$, the correlation coefficient,
$\corr(x_i, x_j) \leq \frac{1}{2k}$, where $k$ is the sparsity of the target,
where $k$ is the sparsity of the target linear function.
%
\item The selection rule is based on \emph{optimisation}, \ie nature selects the
(almost) best mutation from the candidate pool. 
%
\item Evolution is allowed \emph{initialisation}; the starting representation
corresponds to the $0$ vector.
%
\end{enumerate}

Nevertheless, the algorithm is appealing for its simplicity and also for the fact
that it is a \emph{proper} evolutionary mechanism -- it never uses a
representation that is not a $k$-sparse linear function. 

Recall that $f \in  \lin^k_{0, u}$ is the ideal (target)
function.\footnote{Here, we no longer need the fact that each coefficient in the
target linear function has magnitude at least $l$.} Let 
%
\[ R = \{w ~|~ \sparsity(w) \leq k, w_i \in [-B, B] \}, \]
%
where $B = 10uk/\Delta$. Now, define the action of the mutator as follows (we
will define the parameters $\lambda$ and $m$ later): For $w \in R$, 
%%
\begin{enumerate}
%%
\item (Adding) With probability $\lambda$ do the following: Recall $\NZ(w)$
denotes the non-zero entries of $w$. If $|\NZ(w)| < k$, choose $i \in [n]
\setminus \NZ(w)$ uniformly at random. Let $w^\prime$ be such that $w^\prime_j =
w_i$ for $j \neq i$ and $w^\prime_i \in [-B, B]$ uniformly at random. If
$\NZ(w)= k$, let $w^\prime = w$. Then, the multiset $\Neigh(w, \epsilon)$ is
populated by $m$ independent draws from the procedure just described.
%
\item With probability $1 - \lambda$ do the following:
%
\begin{enumerate}
\item (Scaling) With probability $1/2$, choose $\gamma \in [-1, 1]$ uniformly at
random and let $w^\prime = \gamma w$.
%
\item (Adjusting) With probability $1/2$, do the following.  Pick $i \in \NZ(w)$
uniformly at random.  Let $w^\prime$ be such that $w^\prime_j = w_j$ for $j \neq
i$, $w^\prime_i \in [-B, B]$ uniformly at random.
\end{enumerate}
Then, the multiset $\Neigh(w, \epsilon)$ is populated by $m$ independent draws
from the procedure just described.
\end{enumerate}

One thing to note in the above definition is that the mutations produced by the
mutator at any given time are correlated -- \ie they are all either of the kind
that add a new variable, or all of the kind that just manipulate existing
variables.  At a high-level, we prove the success of this mechanism as follows:
\begin{enumerate}
%
\item Using mutations of type ``scaling'' or ``adjusting,'' a representation
that is close to the \emph{best} in the space, \ie $f^S$, is evolved.
%
\item When the representation is (close to) the best possible using current
variables, adding one of the variables that is present in the \emph{ideal
function}, but not in the current representation, results in the greatest
reduction of expected loss. Thus, selection based on optimisation would always
add a variable in $\NZ(f)$. By tuning $\lambda$ appropriately, it is ensured
that candidate mutations that add new variables are never chosen until evolution
has had time to approach the \emph{best} representation using existing
variables.
%
\end{enumerate}

To complete the proof we establish the following claims.

\begin{claim} \label{claim:date} If $\ltwonorm{f^S - w} \leq
\sqrt{\epsilon}/{2k}$, then if $S \subsetneq \NZ(f)$, there exists $i \in \NZ(f)
\setminus S$ and $-B < a < b < B$, such that for any $\gamma \in [a, b]$,
$\loss_{f, D}(w + \gamma e^i) \leq \loss_{f, D}(w) - \epsilon/(4k^2)$ and for
any $j \not\in \NZ(f)$, $\beta \in [-B, B]$, $\loss_{f, D}(w + \beta e^j) \geq
\loss_{f, D}(w + \gamma e^i) + \epsilon/(4k^3)$. Futhermore, $b - a \geq
\sqrt{(k+1) \epsilon}/k$. \end{claim}

\begin{claim} \label{claim:elderberry} With probability at least $(1 - \lambda)
\min\{1/8, \ltwonorm{f^S - w}/(4k^2)\}$, there exists a mutation that reduces the
squared loss by at least $\ltwonorm{f^S - w}^2/12$. \end{claim}

The proofs of Claims~\ref{claim:date} and \ref{claim:elderberry} are not
difficult and are provided in Appendix~\ref{app:greedy}. Based on the above
claims we can prove the following theorem:

\begin{theorem} 

\end{theorem}
\begin{proof}

\end{proof}


%% VK %% In the ensuing discussion, we will prove that for any target function
%% VK %% $w^* \in C^k_{l,u}$, if $w \in R$ is such that
%% VK %% $\lerr_D(w, w^*) = \ltwonorm{w - w^*}^2 > \epsilon$,
%% VK %% then the mutator outputs a mutation which decreases the error by a
%% VK %% non-negligible amount with non-negligible probability.
%% VK %% We will establish our claim by proving the following claims.
%% VK %% 
%% VK %% \begin{enumerate}
%% VK %% \item[Claim A] If $\ltwonorm{w} \ge 2 \ltwonorm{w^*_S}$, then for $M \ge 1$,
%% VK %% with probability at least $1/16 - \lambda/8 \ge $ \eanote{something} the mutator outputs
%% VK %% a mutation that reduces $\lerror$ by at least $\ltwonorm{w - w^*_S}^2 / 12$.
%% VK %% \item[Claim B] If $\ltwonorm{w} \le 2 \ltwonorm{w^*_S}$ and
%% VK %% $\ltwonorm{w - w^*_S} \ge \alpha$, then with probability at least $\alpha$,
%% VK %% there exists a mutation that decreases the $\lerror$ by at least $\alpha$.
%% VK %% \item[Claim C] If $\Delta^2 > 1 - 1/(2 K - 1)$ and
%% VK %% $\ltwonorm{w - w^*_S}^2 < \epsilon$, then $w^\prime$ will be
%% VK %% such that either $\sparseset(w^\prime) = \sparseset(w)$ and
%% VK %% $\ltwonorm{w^\prime - w^*_S}^2 < \epsilon$, or
%% VK %% $\sparseset(w^\prime) \setminus \sparseset(w) = \{i\}$ where $i \in \sparseset(w^*)$
%% VK %% and $w^\prime$ reduces $\lerror$ \eanote{by at least some amount}.
%% VK %% \end{enumerate}
%% VK %% 
%% VK %% To establish Claims A and B above, we follow the lines of reasoning from the
%% VK %% previous section.
%% VK %% For Claim A, we consider the case where selection has chosen the scaling class
%% VK %% of mutants.  With the assumption that $\ltwonorm{w} \ge 2 \ltwonorm{w_S^2}$ and
%% VK %% for each of the generated mutants $\gamma w$ with $\gamma \in [1/2, 3/4]$,
%% VK %% we obtain
%% VK %% 
%% VK %% \[
%% VK %% \ltwonorm{\gamma w - w^*_S}^2 \le \ltwonorm{w - w^*_S}^2 - \frac{1}{12}\ltwonorm{w - w^*_S}^2.
%% VK %% \]
%% VK %% 
%% VK %% For Claim B, \dots
%% VK %% 
%% VK %% For Claim C, in the case that selection chose the adding class of mutations,
%% VK %% we must show that the new index $i \in \sparseset(w^*)$, where
%% VK %% $\{i\} = \sparseset(w^\prime) \setminus \sparseset(w)$.
%% VK %% We show that among the set of mutants output by the mutator, with high probability
%% VK %% there will exist at least one mutant such that it reduces $\lerror$ by something.
%% VK %% \eanote{Explain how this relates to the proof in Donoho and give the correct citation(s)}.
%% VK %% 
%% VK %% Let $\hat{w}$ be the best approximation of $w^*$ such that $\hat{w}_i \in [-B, B]$
%% VK %% for some $i \in [n] \setminus \sparseset(w)$ and $\hat{w}_j = w_j$ for $j \neq i$;
%% VK %% in other words, $\hat{w}$ is the best possible output achieved by mutating $w$ with
%% VK %% one adding step.
%% VK %% In expectation, if $\lznorm{w} = k$, then $M / (n - k)$ of the generated mutants
%% VK %% $v$ will have $\sparseset(v) = \sparseset(\hat{w})$, and each will satisfy
%% VK %% $|v_i - \hat{w}_i| \le \delta$, $\delta < B$, and so
%% VK %% $\ltwonorm{v - \hat{w}} \le \delta$ with probability $\delta/2B$.
%% VK %% Thus in expectation, for $\delta > 2B(n-k) / M$, at least one mutant will be
%% VK %% ``$\delta$-close'' to $\hat{w}$.
%% VK %% Below we argue that the remaining mutants with
%% VK %% $\sparseset(v) \neq \sparseset(\hat{w})$ will not be better than this mutant by
%% VK %% a margin of at least \eanote{fill in this condition}.
%% VK %% 
%% VK %% We must show that $i \in \sparseset(w^*)$ and furthermore that
%% VK %% \eanote{fill in this condition}.
%% VK %% Selection starts with the initial representation, $w=0$.
%% VK %% Without loss of generality, arrange $w^*$ so that $\sparseset(w^*) = [K]$,
%% VK %% in decreasing order of the values $\vert w^*_k \vert \sqrt{\var(X_k)}$.
%% VK %% Initially, the residual $w^* - w$ is $w^*$.
%% VK %% Let $w^*_{\{j\}}$ be the best approximation of $w^*$ with
%% VK %% $\sparseset(w^*_{\{j\}}) = \{j\}$, so $w^*_{\{j\}}$ \dots
%% VK %% With high probability, the mutator generates at least one mutant
%% VK %% $\hat{w}_{\{j\}}$ for each $j \in [n]$ that minimizes the residual to within
%% VK %% $\delta$ of the optimal solution restricted to $\{j\}$, i.e.,
%% VK %% $\ltwonorm{\hat{w}_{\{j\}} - w^*}^2 < \ltwonorm{w^*_{\{j\}} - w^*}^2 + \delta$.
%% VK %% The $\lerror$ of $w^*_{\{j\}}$ is
%% VK %% \eanote{Check the last step below!}
%% VK %% 
%% VK %% \begin{align*}
%% VK %% \ltwonorm{w^*_{\{j\}} - w^*}^2 = \min_{w_{\{j\}}}{\ltwonorm{w_{\{j\}} - w^*}}^2
%% VK %% &= \bigg \Vert e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2} - w^* \bigg \Vert^2 \\
%% VK %% &= \ltwonorm{w^*}^2 - 2 \bigg \langle e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2}, w^* \bigg \rangle + \bigg \Vert e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2} \bigg \Vert^2 \\
%% VK %% &= \ltwonorm{w^*}^2 - \frac{(\ip{e_j}{w^*})^2}{\ltwonorm{e_j}^2} \ge 0.
%% VK %% \end{align*}
%% VK %% 
%% VK %% \noindent Note that
%% VK %% 
%% VK %% \[
%% VK %% \frac{(\ip{e_j}{w^*})^2}{\ltwonorm{e_j}^2}
%% VK %% = \frac{(\E[(e_j \cdot X)(w^* \cdot X)])^2}{\E[(e_j \cdot X)(e_j \cdot X)]}
%% VK %% = \frac{(\E[X_j (w^* \cdot X)])^2}{\E[X_j^2]}
%% VK %% = \frac{(\sum_{k=1}^K w^*_k \E[X_j X_k]))^2}{\var(X_j)}
%% VK %% \]
%% VK %% 
%% VK %% For selection to choose a mutant $\hat{w}_{\{j\}}$ with
%% VK %% $j \in \sparseset(w^*) = [K]$, it must be the case that for any $i > K$,
%% VK %% 
%% VK %% \[
%% VK %% \bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
%% VK %% > \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert + \eanote{~something}.
%% VK %% \]
%% VK %% 
%% VK %% We construct lower and upper bounds for the left- and right-hand sides,
%% VK %% respectively, making use of the fact that
%% VK %% $|\corr(X_i, X_j)| \leq 1 - \Delta^2$ from Lemma~\ref{} and also
%% VK %% that we arranged the $\vert w^*_k \vert \sqrt{\var(X_k)}$ in decreasing order.
%% VK %% On the left,
%% VK %% 
%% VK %% \begin{align*}
%% VK %% \bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
%% VK %% &= \frac{\vert\sum_{k=1}^K w^*_k \E[X_1 X_k]\vert}{\sqrt{\var(X_1)}} \\
%% VK %% &\ge \frac{\vert w^*_1 \E[X_1 X_1]\vert}{\sqrt{\var(X_1)}} - \frac{\sum_{k=2}^K \vert w^*_k \E[X_1 X_k] \vert}{\sqrt{\var(X_1)}} \\
%% VK %% &= \vert w^*_1 \vert \sqrt{\var(X_1)} - \sum_{k=2}^K \vert w^*_k \vert \sqrt{\var(X_k)}~\vert \corr(X_1, X_k) \vert \\
%% VK %% &\ge \vert w^*_1 \vert \sqrt{\var(X_1)}(1 - (1 - \Delta^2) (K-1))
%% VK %% \end{align*}
%% VK %% 
%% VK %% \noindent On the right,
%% VK %% 
%% VK %% \begin{align*}
%% VK %% \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert
%% VK %% = \frac{\vert\sum_{k=1}^K w^*_k \E[X_i X_k]\vert}{\sqrt{\var(X_i)}}
%% VK %% \le \sum_{k=1}^K \vert w^*_k \vert \frac{\vert \E[X_i X_k]\vert}{\sqrt{\var(X_i)}}
%% VK %% &= \sum_{k=1}^K \vert w^*_k \vert \sqrt{\var(X_k)} \corr(X_i, X_k) \\
%% VK %% &\le \vert w^*_1 \vert \sqrt{\var(X_1)} (1 - \Delta^2) K.
%% VK %% \end{align*}
%% VK %% 
%% VK %% \noindent Putting these bounds together now gives the requirement
%% VK %% 
%% VK %% \[
%% VK %% \bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
%% VK %% \ge \vert w^*_1 \vert \sqrt{\var(X_1)}(1 - (1 - \Delta^2) (K-1))
%% VK %% > \vert w^*_1 \vert \sqrt{\var(X_1)} (1 - \Delta^2) K
%% VK %% \ge \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert,
%% VK %% \]
%% VK %% 
%% VK %% \noindent and so
%% VK %% 
%% VK %% \begin{align*}
%% VK %% 1 - (1 - \Delta^2) (K-1) = 2 - K + \Delta^2 K - \Delta^2 &> (1 - \Delta^2) K \\
%% VK %% %\Rightarrow & \qquad 1 + (1 - \Delta^2) > 2 (1 - \Delta^2) K
%% VK %% %\qquad \Rightarrow \qquad K < \frac{1}{2} \biggl( 1 + \frac{1}{1 - \Delta^2} \biggr)
%% VK %% \Rightarrow \qquad \Delta^2 &> 1 - \frac{1}{2 K - 1}
%% VK %% \end{align*}
%% VK %% 
%% VK %% \noindent  This is the condition for $\Delta^2$ in Claim C.
%% VK %% \eanote{Finally, argue that subsequent steps work too.}
%% VK %% \eanote{And then we should say something about the CS analog.}
