We first provide an overview of the evolvability framework of
\cite{Valiant:2009-evolvability}. The description here contains differs slightly
from Valiant's orginal formulation (see also \cite{Feldman:2008-evolvability,
Feldman:2009-robustness, Valiant:2012-real, Kanade:2012-thesis}).

\subsection{Valiant's Evolvability Framework}

Let $X$ denote a set of instances, \eg $X = \reals^n$ or $X = \{0, 1\}^n$. We
assume that the length of representing each $x \in X$ is captured by the
parameter $n$. To avoid excessive notation, we will keep this size parameter
implicit in our description of the model. Let $D$ be a distribution over $X$.
Each $x \in X$ can be thought of as the description of an environmental setting,
the inputs to the circuit of an organism. $D$ denotes the distribution over the
possible environmental settings an organism may experience in a lifetime. Let $f
: X \rightarrow Y$ (typically $Y = \reals$ or $Y = \{0, 1\}$) denote the ideal
function, the (possibly hypothetical) best behaviour in each possible
environmental setting.

\subsubsection*{Representations}

A creature is a string representation that encodes an efficiently computable
function $r : X \rightarrow Y$, \ie there is an efficient Turing Machine that
given the description string $\langle r \rangle$ and $x \in X$, outputs $r(x)$.  

In this work, our focus is characterizing different evolutionary mechanisms
based on the complexity of representations used. The notion is similar in spirit
to the \emph{proper} vs. \emph{improper} learning question in computational
learning theory. The complexity of a representation is measured by the function
it computes.  Let $H : X \rightarrow Y$ be a class of functions. Let $R
\subseteq \{0, 1\}^*$.  We say that $R$ \emph{represents} $H$, if there is a map
$\sigma : R \rightarrow H$ and there exists an \emph{efficient} Turing machine
that given input $r \in R$ and $x \in X$, outputs $(\sigma(r))(x)$. Henceforth,
by abuse of notation we will use $r$ to denote both the representation and the
function it computes.

\subsubsection*{Evolutionary Algorithms}

The performance of a representation $r$ is measured using a loss function, $\ell
: Y \times Y \rightarrow \reals^+$, such that $\ell(y, y) = 0$. For a function
$g : X \rightarrow Y$, define the expected loss with respect to the ideal
function $f : X \rightarrow Y$, under distribution $D$, as $\loss_{f, D} = \E_{x
\sim D}[\ell(g(x), f(x))]$\footnote{This definition does not require the
expected loss to be bounded, but we'll mainly be interested in situations when
that is the case.}. The goal of evolution is to reach some representation $r^*$
such that $\loss_{f, D}(g) < \epsilon$. In the following discussion, we use the
notation: $f$ the ideal function, $\epsilon$ the target accuracy, $D$ the target
distribution over $X$ and $\loss_{f, D}(f)$ the expected loss function. \medskip \\
%
\noindent{\bf Mutator}: A mutator for a class of representations $R$, $\Mut$, is
a polynomial-time randomised Turing machine that takes as in put a
representation $r \in R$ and accuracy parameter $\epsilon$ and outputs a
multiset $\Neigh(r, \epsilon) \subseteq R$. \medskip \\
%
\noindent{\bf Selection}: (Natural) Selection is based on empirical performance
of each representation. Let $s : R \times [0, 1] \rightarrow \naturals$ be a
sample size function. First, the mutation algorithm, $\Mut(r, \epsilon)$ is run
to produce mutliset $\Neigh(r, \epsilon)$. Then an i.i.d. sample $\langle x_i
\rangle_{i=1}^s$ is drawn from the distribution $D$ over $X$, where $s = s(r,
\epsilon)$.  Denote the empirical performance of each $r^\prime \in \Neigh(r,
\epsilon) \cup \{r \}$ as
%
\[ \hat{\loss}_{f, D}(g) = \frac{1}{s}\sum_{i=1}^s \ell(r^\prime(x_i), f(x_i)) \]
%
Finally, let $t : R \times [0, 1] \rightarrow \reals$ be a tolerance function.
Two possible selection mechanisms are considered.
\begin{enumerate}
\item {\bf Selection based on beneficial and neutral mutations}: Let 
%
\[ \Bene = \{r^\prime \in \Neigh(r, \epsilon) ~|~ \hat{\loss}_{f, D}(r^\prime) \leq
\hat{\loss}_{f, D}(r) - t(r, \epsilon) \} \]  
%
denote the set of beneficial mutations and let 
%
\[ \Neut = \{r^\prime \in \Neigh(r, \epsilon) ~|~ |\hat{\loss}_{f, D}(r^\prime) -
\hat{\loss}_{f, D}(r)| <  t(r, \epsilon) \} \]
%
denote the neutral mutations, with respect to tolerance function $t$. Selection
operates as follows: if $\Bene \neq \emptyset$, $r^\prime$ is randomly selected
from $\Bene$ as the surviving creature at the next generation.  If $\Bene =
\emptyset$ and $\Neut \neq\emptyset$, then $r^\prime$ is selected randomly from
$\Neut$ as the surviving creature at the next generation.  Otherwise, $\bot$ is
produced signifying failure of evolution.
%
\item {\bf Selection based on optimisation}: Let $\widehat{\opt} =
\displaystyle\min_{r^\prime \in \Neigh(r, \epsilon)} \hat{\loss}_{f, D}(r^\prime)$.
If $\opt < \hat{\loss}_{f, D}(r) + t(r, \epsilon)$, then $\bot$ is produced
signifying failure of evolution.  Otherwise, let $\best = \{ r^\prime \in
\Neigh(r, \epsilon) ~|~ \hat{\loss}_{f, D}(r^\prime) \leq \opt + t(r, \epsilon) \}$.
Then $r^\prime$ is chosen from $\best$ randomly as the surviving creature at the
next generation.
\end{enumerate}

We denote a selection rule by $\Sel[R, \Mut, s, t]$, which may be one of the two
types described above. For $\Sel$ to be feasible we require that the size
function $s$ is polynomially (in $n$ and $1/\epsilon$) bounded and that there
exists polynomials $p_1(n, 1/\epsilon)$ and $p_2(n, 1/\epsilon)$, such that
$1/p_1(n, 1/\epsilon) \leq t(r, \epsilon) \leq p_2(n, 1/\epsilon)$ for every $r
\in R$. \medskip \\
%
\noindent {\bf Evolutionary Algorithm}: An evolutionary algorithm $\evalg$ is a
tuple $(R, \Mut, s, t, \Sel)$. When $\evalg$ is run starting from $r_0 \in R$
with respect to distribution $D$ over $X$, ideal function $f : X \rightarrow Y$,
loss function $\ell$ and parameter $\epsilon$, a sequence $r_0, r_1, r_2,
\ldots$ is produced, where $r_i$ is obtained from $r_{i-1}$ through application
of the selection rule $\Sel[R, \Mut, s, t]$. If $r_i = \bot$ for some $i$, we
consider evolution as halted and $r_j = \bot$ for $j > i$. We say that $\evalg$
succeeds at generation $g$, if $g$ is the smallest for which the expected loss,
$\loss_{f, D}(r_g) \leq \epsilon$.

\begin{definition}[Evolvability \cite{Valiant:2009-evolvability}] We say that a concept class $C$ is evolvable under a class of
distributions $\Dists$ using a representation class $R$, if there exists an
evolutionary algorithm $\evalg = (R, \Mut, s, t, \Sel)$, such that for every $D
\in \Dists$, every $f \in C$, and every $\epsilon > 0$, every $r_0 \in R$, with
probability at least $1 - \epsilon$, $\evalg$ run starting from $r_0$ with
respect to $f, D, \epsilon$, produces $r_g$ for which $\loss_{f, D}(r_g) <
\epsilon$. Furthermore, $g$ is bounded by a polynomial in $n, 1/\epsilon$.
\end{definition}

The definition presented above varies slightly from the definition of Valiant,
in the sense that we explicitly focus on the class of representations $R$ used
by the evolutionary algorithm. As discussed in the introduction, we focus on
concept classes where each function depends on \emph{few} (constant) input
variables\footnote{These functions have been referred to as juntas in the theory
literature. We avoid using this nomenclature as we restrict our attention to
specific functional forms, such as linear functions, with $k$ relevant
variables.}. 

\subsubsection{Strictly Beneficial Neighbourhoods}

Kanade \etal defined the notion of strictly beneficial neighbourhood
mutators~\cite{KVV:2010-drift}. At a high-level, these are mutation algorithms
that guarantee that at least one of the mutations produced will be (noticeably)
better than the current representation. It follows easily that if such a mutator
exists for some concept class $C$, then the class $C$ is
evolvable~\cite{KVV:2010-drift}. Formally, we can define.

\begin{definition}[Strictly Beneficial Neighbourhood Mutator] For a concept
class $C$, class of distributions $\Dists$, and $b : [0, 1] \rightarrow \reals$, we
say that a mutator $\Mut$ is a $b$-strictly beneficial neighbourhood mutator
over representation class $R$, if for every $r \in R$, $f \in C$, $D \in
\Dists$, and $\epsilon > 0$, if $\Neigh(r, \epsilon)$ is the output produced by
running the mutator $\Mut$ with $r$ and $\epsilon$ as inputs, then with
probability $1$, there exists $r^\prime \in \Neigh(r, \epsilon)$, such that
$\loss_{f, D}(r^\prime) < \loss_{f, D}(r)$.  \end{definition}

The purpose of defining strictly beneficial neighbourhood mutators is that
together with concentration bounds relating empirical loss to the true expected
loss, the existence of such mutators implies evolvability of the corresponding
class. Let $R$ be a class of representations, $C$ a concept class, $\Dists$ a
class of distributions and $\ell : Y \times Y \rightarrow \reals^+$ a loss
function such that for every $f \in C$, $D \in \Dists$ and $r \in R$, the
expected loss $\loss_{f, D}(r)$ is bounded and further that if $x_1, \ldots,
x_s$ are i.i.d. samples drawn from $D$, the following is true:
%
\[ \Pr\left[ |\frac{1}{s} \sum_{i=1}^s \ell(r(x_i), f(x_i)) - \loss_{f, D}(r)|
\geq \tau \right] \leq \exp(c_1 s^{c_2} \tau^{c_3}), \]
%
for absolute constants $c_1, c_2, c_3$. In such a case, we say that the loss
function with respect to $C$, $R$, $\Dists$ is bounded has exponential
concentration. 

\begin{theorem}[(Theorem 8~\cite{KVV:2010-drift})] \label{thm:benefit2evolve}
Let $C$ be a concept class, $\Dists$ a class of distributions. For a class of
representations, $R$, loss function, $\ell$, if there exists a strictly
beneficial neighbourhood mutator with benefit $b$, then $C$ is evolvable under
distributions from $\Dists$, with respect to loss function $\ell$ using the
representation class $R$.  \end{theorem}

The proof of the above Theorem essentially appears in \cite{KVV:2010-drift}, who
also show that such an algorithm is robust to some modest (inverse polynomial)
drift in the target function. For the purpose of this paper, we will show the
existence of strictly beneficial neighbourhood mutators and appeal to
Theorem~\ref{thm:benefit2evolve}.

\subsection{Sparse Linear Functions} 

Our main result in this paper considers the class of sparse linear functions.
We represent a linear functions from $\reals^n \rightarrow \reals$ by a vector
$w \in \reals^n$, where $x \mapsto w \cdot x$.  For a vector $w \in \reals^n$,
$\lznorm{w}$ is the number of non-zero elements of $w$. 

For any $0 < l < u$ and integer $k$. Define the class of linear functions:
\[
\lin^k_{l, u} = \{ x \mapsto w \cdot x ~|~ \lznorm{w} \leq k, \forall i,
w_i = 0 \mbox{ or } l \leq |w_i| \leq u \}
\]
Thus, $\lin^k_{l, u}$ is the class of sparse linear functions, where the
``influence'' of each variable is upper and lower bounded\footnote{We do not use
the word influence in the precise technical sense here}.

Let $D$ be a distribtion over $\reals^n$. For $w, w^\prime$, define the inner
product $\ip{w}{w^\prime}_D = \E_{x \sim D}[(w \cdot x) (w^\prime \cdot x)]$.
(Note that $w \cdot x = \sum_{i = 1}^n w_i x_i$ denotes the standard dot product
in $\reals^n$.) In the rest of this paper, we use $\ltwonorm{w}_D$ to denote
$\sqrt{\ip{w}{w}_D}$. (To avoid confusion, whenever necessary, we
will refer to the quantity $\sqrt{\sum_{i} w_i^2}$ explicitly if we mean the standard
Euclidean norm.) When the distribution $D$ is clear from context we will drop
the subscript.



\subsubsection*{Pretty Distributions}

We focus on distributions over $\reals^n$ which have certain nice properties. A
distribution $D$ over $\reals^n$ is said to be $c^2$-sub-Gaussian if for any $w
\in \reals^n$ satisfying $\sum_{i=1}^n w_i^2 = 1$, it is the case that
$\E_{x~D}[ \exp((w \cdot x)t)] \leq \exp(c^2 t^2 /2)$.\footnote{Here we are
using a multidimensional analogue of the standard definition of sub-Gaussian
distributions, \ie the distribution projected along any direction is
sub-Gaussian.} Furthermore, we assume that the distribution is centered, \ie
$\E[x] = 0$ and the variance in each component bounded $\E[x_i^2] \leq 1$. The
last property, we require of a distribution is that no variable is $x_i$ is
completely determined by the remaining variables, which we denote by $x_{-i}$,
or more formally, the variance, $\var(x_i ~|~ x_{i-1}) \geq 2 \Delta^2$, for
some $\Delta > 0$.\footnote{For the purposes of this paper, it suffices if this
holds only in expectation, \ie $\E_{x_{-i}}[\var(x_i ~|~ x_{i-1})] \geq 2
\Delta^2$.} We argue that this is a natural \emph{smoothness} condition on the
distribution. For example, starting from any distribution $D$ over $\reals^n$,
convolution with a spherical normal distribution with variance $2 \Delta^2$
results in a distribution having such a property. We define this class of
distributions formally, and for a lack of a better word call them \emph{pretty}
distributions.

\begin{definition} A distribution $D$ over $\reals^n$ is a $(c^2,
2\Delta^2)$-pretty distribution if the following hold:
\begin{enumerate}
\item $\E[x] = 0$
\item For $1 \leq i \leq n$, $\E[x_i^2] \leq 1$
\item $D$ is $c^2$-sub-Gaussian
\item $\E_{x_{-i}}[\var(x_i ~|~ x_{-i})] \geq 2\Delta^2$.
\end{enumerate}
\end{definition}

We first show a few useful properties of these distributions. The first is a
concentration bound for i.i.d. samples drawn from the distribution. The proof of
the lemma follows directly from the bound on the moment generating function for
sub-Gaussian distributions.

\begin{lemma}
Let $w \in \reals^n$. Suppose $x_1, \ldots, x_m$ are drawn independently from
a $(c^2, 2 \Delta^2)$-pretty distribution $D$. Then,
\[ 
\Pr[ ] \leq \exp( ).
\]
\end{lemma}

Another useful property of nice distributions is that correlation between any
pair of variables is bounded away from $1$ and $-1$.  Recall that for random
variables $X, Y$, the correlation, $\corr(X, Y) = \E[(X - \E[X]) (Y -
E[Y])]/\sqrt{\var(X) \var(Y)}$. 

\begin{lemma} Let $D$ be a $(c^2, 2 \Delta^2)$-pretty distribution.\footnote{We
only need the $2 \Delta^2$-smoothness condition for this Lemma to hold.} Then
for all $i, j$, $|\corr(x_i, x_j)| \leq 1 - \Delta^2$.
\end{lemma}
\begin{proof}

\end{proof}

Finally, for linear functions $w : x \mapsto w \cdot x$, let $\NZ(w) = \{ i ~|~
w_i \neq 0 \}$ denote the non-zero variables in $w$ and $\sparsity(w) =
|\NZ(w)|$. Then, we have the following Lemma.

\begin{lemma} Let $D$ be a $(c^2, 2 \Delta^2)$-pretty distribution and $w : x
\mapsto w \cdot x$ denote a linear function. Then the following are true:
\begin{enumerate}
\item For any $i$, $w^2_i \leq \ip{w}{w}/\Delta^2$
\item There exists an $i$ such that $w^2_i \leq \frac{\ip{w}{w}}{|\NZ(w)| \Delta^2}$.
\end{enumerate}
\end{lemma}
\begin{proof}

\end{proof}
