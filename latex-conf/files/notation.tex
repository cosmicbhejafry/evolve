We first provide an overview of the evolvability framework of
Valiant~\cite{Valiant:2009-evolvability}. The description here differs slightly
from Valiant's original formulation and includes some subsequent extensions (for
more details the reader is referred to
\cite{Valiant:2009-evolvability,Feldman:2008-evolvability,
Feldman:2009-robustness, Valiant:2012-real, Kanade:2012-thesis}).

\vfill\eject
\subsection{Valiant's Evolvability Framework}
\label{sec:notation-model}

Let $X$ denote a set of instances, \eg $X = \reals^n$ or $X = \{0, 1\}^n$. We
assume that the representation length of each $x \in X$ is captured by the
parameter $n$. To avoid excessive notation, we will keep this size parameter
implicit in our description of the model. Let $D$ be a distribution over $X$.
Each $x \in X$ can be thought of as the description of an environmental setting,
the inputs to any circuit of an organism. $D$ denotes the distribution over the
possible environmental settings an organism may experience in a lifetime. Let $f
: X \rightarrow Y$ (typically $Y = \reals$ or $Y = \{0, 1\}$) denote the
\emph{ideal function}, the best behavior in each possible environmental
setting.

\subsubsection*{Representations}

A creature is a string representation that encodes an efficiently computable
function $r : X \rightarrow Y$, \ie there is an efficient Turing Machine that,
given the description string $\langle r \rangle$ and $x \in X$, outputs $r(x)$.  

In this work, our focus is characterizing different evolutionary mechanisms
based on the complexity of representations used. 
% The notion is similar in spirit to the \emph{proper} vs. \emph{improper}
% learning question in computational learning theory. 
The complexity of a representation is measured by the function it computes.  Let
$H : X \rightarrow Y$ be a class of functions. For $R \subseteq \{0, 1\}^*$,  we
say that $R$ \emph{represents} $H$, if there is a map, $\sigma : R \rightarrow
H$, and if there exists an \emph{efficient} Turing machine that, given input $r
\in R$ and $x \in X$, outputs $(\sigma(r))(x)$. Henceforth, by abuse of notation
we will use $r$ to denote both the representation and the function it computes,
$\sigma(r)$. 

\subsubsection*{Evolutionary Algorithms}

The performance of a representation $r$ is measured using a loss function $\ell
: Y \times Y \rightarrow \reals^+$, such that $\ell(y, y) = 0$. For a function
$g : X \rightarrow Y$, define the expected loss with respect to the ideal
function $f : X \rightarrow Y$, under distribution $D$, as $\loss_{f, D}(g) =
\E_{x \sim D}[\ell(g(x), f(x))]$.\footnote{This definition does not require the
expected loss to be bounded, but we will mainly be interested in situations when
that is the case.} The goal of evolution is to reach some representation $r^*$
such that $\loss_{f, D}(r^*) < \epsilon$. In the following discussion, we use
the notation: $f$ the ideal function, $\epsilon$ the target accuracy, $D$ the
target distribution over $X$ and $\loss_{f, D}(g)$ the expected loss function.  \medskip \\
%
\noindent{\bf Mutator}: A mutator $\Mut(r, \epsilon)$, for a set of representations $R$, is
a polynomial-time randomized Turing machine that takes as input a representation
$r \in R$ and accuracy parameter $\epsilon$ and outputs a multiset $\Neigh(r,
\epsilon) \subseteq R$. The running time requirement on $\Mut$ also ensures that
$|\Neigh(r, \epsilon)|$ is polynomially bounded. \medskip \\
%
\noindent{\bf Selection}: (Natural) Selection is based on the empirical performance
of each representation. Let $s : R \times [0, 1] \rightarrow \naturals$ be a
sample size function. First, the mutation algorithm, $\Mut(r, \epsilon)$, is run
to produce multiset $\Neigh(r, \epsilon)$. Then, an i.i.d. sample $\langle x^i
\rangle_{i=1}^s$ is drawn from the distribution $D$ over $X$, where $s = s(r,
\epsilon)$.  Denote the empirical performance of each $r^\prime \in \Neigh(r,
\epsilon) \cup \{r \}$ as
%
\[ \hat{\loss}_{f, D}(r^\prime) = \frac{1}{s}\sum_{i=1}^s \ell(r^\prime(x^i),
f(x^i)) \]
%
Finally, let $t : R \times [0, 1] \rightarrow \reals$ be a tolerance function.
Two possible selection mechanisms are considered.
\begin{enumerate}
\item {\bf Selection based on beneficial and neutral mutations} ($\bnsel$): Let 
%
\[ \Bene = \{r^\prime \in \Neigh(r, \epsilon) ~|~ \hat{\loss}_{f, D}(r^\prime) \leq
\hat{\loss}_{f, D}(r) - t(r, \epsilon) \} \]  
%
denote the set of beneficial mutations and let 
%
\[ \Neut = \{r^\prime \in \Neigh(r, \epsilon) ~|~ |\hat{\loss}_{f, D}(r^\prime) -
\hat{\loss}_{f, D}(r)| <  t(r, \epsilon) \} \]
%
denote the neutral mutations, with respect to tolerance function $t$. Both
$\Bene$ and $\Neut$ are treated as multisets (the multiplicity of any
representation is the same as that in $\Neigh(r, \epsilon)$). Selection
operates as follows: if $\Bene \neq \emptyset$, $r^\prime$ is randomly selected
from $\Bene$ as the surviving creature at the next generation.  If $\Bene =
\emptyset$ and $\Neut \neq\emptyset$, then $r^\prime$ is selected randomly from
$\Neut$ as the surviving creature at the next generation.  Otherwise, $\bot$ is
produced signifying failure of evolution.
%
\item {\bf Selection based on optimization} ($\optsel$): Let $\widehat{\opt} =
\displaystyle\min_{r^\prime \in \Neigh(r, \epsilon)} \hat{\loss}_{f,
D}(r^\prime)$.  If $\widehat{\opt} > \hat{\loss}_{f, D}(r) + t(r, \epsilon)$,
then $\bot$ is produced signifying failure of evolution.  Otherwise, consider
the multiset, 
\[ \best = \{ r^\prime \in \Neigh(r, \epsilon) ~|~ \hat{\loss}_{f,
D}(r^\prime) \leq \widehat{\opt} + t(r, \epsilon) \},\] 
and then $r^\prime$ is
chosen from $\best$ randomly as the surviving creature at the next generation.
\end{enumerate}

\noindent Thus, while the selection rule $\bnsel$ only chooses some beneficial
(or at least neutral) mutation, $\optsel$ aggressively picks the (almost) best
mutation from the available pool. \medskip

We denote by $r^\prime \leftarrow \Sel[R, \Mut, s, t](r, \epsilon)$ the fact
that $r^\prime$ is the surviving creature in the next generation after one
mutation and selection operation on the representation $r$ and accuracy
parameter $\epsilon$. Here, $\Sel$ may be one of the two selection rules
described above. For $\Sel$ to be feasible we require that the size function $s$
is polynomially bounded (in $n$ and $1/\epsilon$) and that the inverse of the tolerance
function $t$ is polynomially sandwiched, \ie there exists polynomials $p_1(n,
1/\epsilon)$ and $p_2(n, 1/\epsilon)$ such that $1/p_1(n, 1/\epsilon) \leq t(r,
\epsilon) \leq 1/p_2(n, 1/\epsilon)$ for every $r \in R$ and $\epsilon > 0$.
\medskip \\
%
\noindent {\bf Evolutionary Algorithm}: An evolutionary algorithm $\evalg$ is a
tuple $(R, \Mut, s, t, \Sel)$. When $\evalg$ is run starting from $r_0 \in R$
with respect to distribution $D$ over $X$, ideal function $f : X \rightarrow Y$,
loss function $\ell$ and parameter $\epsilon$, a sequence $r_0, r_1, r_2,
\ldots$ is produced, where $r_i \leftarrow \Sel[R, \Mut, s, t](r_{i - 1},
\epsilon)$. If $r_i = \bot$ for some $i$, we consider evolution as halted and
$r_j = \bot$ for $j > i$. We say that $\evalg$ succeeds at generation $g$, if
$g$ is the smallest index for which the expected loss $\loss_{f, D}(r_g) \leq
\epsilon$.

\begin{definition}[Evolvability \cite{Valiant:2009-evolvability}] We say that a
concept class $C$ is evolvable with respect to loss function $\ell$ and selection
rule $\Sel$, under a class of distributions $\Dists$ using a representation
class $H$, if there exists a representation scheme $R \subseteq \{0, 1\}^*$,
such that $R$ represents $H$, and there exists an evolutionary algorithm $\evalg
= (R, \Mut, s, t, \Sel)$, such that for every $D \in \Dists$, every $f \in C$,
every $\epsilon > 0$, and every $r_0 \in R$, with probability at least $1 -
\epsilon$, $\evalg$ run starting from $r_0$ with respect to $f, D, \ell,
\epsilon$, produces $r_g$ for which $\loss_{f, D}(r_g) < \epsilon$.
Furthermore, the number of generations $g$ required for evolution to succeed
should be bounded by a polynomial in $n$ and $1/\epsilon$.  \end{definition}

\begin{remark} If the evolutionary algorithm succeeds only for a specific
starting representation $r_0$, we say $C$ is evolvable with
\emph{initialization}. \end{remark}

\begin{remark} If the functions in concept class $C$ depend only on $k$
variables, we say the evolutionary algorithm is attribute-efficient, if the size
function, $s$, is polylogarithmic in $n$, and polynomial in $k$ and
$1/\epsilon$, and the number of generations, $g$, is polynomial in $k$ and
$1/\epsilon$, but does not depend on $n$.
\end{remark}

The definition presented above varies slightly from the definition of Valiant,
in the sense that we explicitly focus on the complexity of representations used
by the evolutionary algorithm. As discussed in the introduction, we focus on
concept classes where each function depends on \emph{few} (constant) input
variables.\footnote{These functions have been referred to as juntas in the
theory literature. We avoid using this nomenclature as we restrict our attention
to specific functional forms, such as linear functions, with $k$ relevant
variables.} 

%% Consider adding this comment later in Section 3.1
% \begin{remark} Valiant~\cite{Valiant:2009-evolvability} showed that selection
% using optimisation was not more powerful than selection using beneficial and
% neutral mutations alone. This equivalence holds if one allows arbitrary
% representation classes. It is not necessary that such an equivalence hold when
% the representation class is restricted. In particular, the proof of equivalence
% requires memory, \ie part of the representation stores historical
% information and does not necessarily contribute to the function being computed.
% We discuss this issue a bit further in Section~\ref{sec:sparse-linear-greedy}.
% \end{remark}

\subsection{Sparse Linear Functions} 
\label{sec:notation-class}

Our main result in this paper concerns the class of sparse linear functions.  We
represent a linear function from $\reals^n \rightarrow \reals$ by a vector $w
\in \reals^n$, where $x \mapsto w \cdot x$.  For a vector $w \in \reals^n$,
$\lznorm{w}$ is the number of non-zero elements of $w$.

For any $0 \leq l < u$ and integer $k$, define the class of linear functions:
\[
\lin^k_{l, u} = \{ x \mapsto w \cdot x ~|~ \lznorm{w} \leq k, \forall i,
w_i = 0 \mbox{ or } l \leq |w_i| \leq u \}
\]
Thus, $\lin^k_{l, u}$ is the class of $k$-sparse linear functions, where the
``influence'' of each variable is upper and lower bounded.\footnote{We do not
use the word ``influence'' in the precise technical sense here.}
\todoea{Why ``influence'' rather than simply magnitude?}

Let $D$ be a distribution over $\reals^n$. For $w, w^\prime \in \reals^n$, define the inner
product $\ip{w}{w^\prime} = \E_{x \sim D}[(w \cdot x) (w^\prime \cdot x)]$,
where $w \cdot x = \sum_{i = 1}^n w_i x_i$ denotes the standard dot product in
$\reals^n$. In this paper, we use $\ltwonorm{w}$ to denote $\sqrt{\ip{w}{w}}$
(and not $\sqrt{\sum_{i} w_i^2}$). To avoid confusion, whenever necessary, we
will refer to the quantity $\sqrt{\sum_{i} w_i^2}$ explicitly if we mean the
standard Euclidean norm. 
%
%When the distribution $D$ is clear from context we will drop the subscript.
%

\subsubsection*{Distribution Classes}

We use two classes of distributions for our results in this paper. We define
them formally here. \medskip 

\noindent{\bf Smooth Bounded Distributions}: We consider the class of smooth
bounded distributions over $\reals^n$. The concept of smoothed analysis of
algorithms was introduced by Spielman and Teng~\cite{ST:2004} and recently the
idea has been used in learning theory~\cite{KST:2009,KKM:2013}. We consider
distributions that are bounded and have $0$ mean. Formally,
distributions we consider are defined as:

\begin{definition}[$\Delta$-Smooth $G$-Nice Distribution]
\label{defn:afghanistan} A distribution $D$ is a $\Delta$-smooth $G$-nice
distribution if it is obtained as follows. Let $\tilde{D}$ be some distribution
over $\reals^n$, and let $U^n_a$ denote the uniform distribution over $[-a,
a]^n$. Then $D = \tilde{D} * U^n_{\sqrt{3}\Delta}$ is obtained by the convolution of
$\tilde{D}$ with $U^n_{\sqrt{3} \Delta}$.\footnote{We could perform convolution
with a spherical Gaussian distribution, however, this would make the resulting
distribution unbounded. All results in this paper hold if we work with
sub-Gaussian distributions and consider convolution with a spherical Gaussian
distribution with variance $\Delta^2$.  In this case, we would be required to use
Chebychev's inequality rather than Hoeffding's bound to show that the empirical
estimate is close to the expected loss with high probability.} Furthermore, $D$
satisfies the following:
%%
\begin{enumerate}
\item $\E_D[x] = 0$
\item For all $i$, $\E_D[x_i^2] \leq 1$
\item For every $x$ in the support of $D$, $\sum_{i = 1}^n x_i^2 \leq G^2$
\end{enumerate}
\end{definition}

\noindent{\bf Incoherent Distributions}: We also consider \emph{incoherent}
distributions.\footnote{This terminology is adapted from incoherence of
matrices, \eg see~\cite{Donoho:2009-sparse}.} For a distribution $D$ over
$\reals^n$, the coherence is defined as $\max_{i, j} \corr(x_i, x_j)$, where
$\corr(x_i, x_j)$ is the correlation between $x_i$ and $x_j$. Again, we consider
bounded distributions with zero mean. We also require the variance to be upper
and lower bounded in each dimension. Formally, the distributions we consider are
defined as:

\begin{definition}[$\mu$-Incoherent $(\Delta, G)$-Nice Distribution]
\label{defn:bhutan} A distribution $D$ is a $\mu$-incoherent $(\Delta, G)$-nice
distribution if the following hold:
\begin{enumerate}
\item $\E_D[x] = 0$
\item For all $i$, $\Delta^2 \leq \E_D[x_i^2] \leq 1$
\item For all $i$, $j$, $\max_{i, j} \corr(x_i, x_j) \leq \mu$
\item For all $x$ in the support of $D$, $\sum_{i=1}^n x_i^2 \leq G^2$
\end{enumerate}
\end{definition}

%Let $\tilde{D}$ be an arbitrary bounded distribution on $\reals^n$ satisfying
%$\E_{\tilde{x} \sim \tilde{D}}[\tilde{x}]=0$ and, for all $i$,
%$\E[\tilde{x}_i^2] \leq 1 - \Delta^2$.  Let $U^n_{\sqrt{3}\Delta}$ denote the
%uniform distribution over $[-\sqrt{3} \Delta, \sqrt{3}\Delta]$, then $D =
%\tilde{D}* U^n_{\sqrt{3} \Delta}$, the convolution of $\tilde{D}$ and
%$U^n_{\sqrt{3}\Delta}$, is a smooth distribution
% Alternatively, one may view drawing points from $D$ as follows: pick $\tilde{x}
% \sim \tilde{D}$, draw $\eta \in [-\sqrt{3} \Delta, \sqrt{3} \Delta]^n$ uniformly
% at random, and output $x = \tilde{x} + \eta$. We call such a distribution a
%$\Delta$-smooth bounded distribution. Note that $\E_{x \sim D} = 0$ and
% $\E[x_i^2] \leq 1$.

We say a linear function represented by $w \in \reals^n$ is $W$-bounded if
$\sum_{i=1}^n w_i^2 \leq W^2$. We use the notation $w(x) = w \cdot x$. Suppose
$f, w$ are $W$-bounded linear functions, and distribution $D$ is such that for every
$x$ in the support of $D$, $\sum_{i=1}^n x_i^2 \leq G^2$.  We consider the
squared loss function, which for $y, y^\prime \in \reals$ is $\ell(y^\prime, y) =
(y^\prime - y)^2$.  Then, for any $x$ in the support of $D$, $\ell(f(x), w(x))
\leq 4 W^2G^2$. Thus, standard Hoeffding bounds imply that
%when $\ell$ is the squared loss function, for the class of $\Delta$-smooth
%$G$-bounded distributions, and $W$-bounded linear functions, 
if $\langle x^i \rangle_{i=1}^s$ is an i.i.d. sample drawn from $D$, then
%%
\begin{align}
\Pr\left[\left| \frac{1}{s} \sum_{i=1}^s \ell(w(x^i), f(x^i)) - \loss_{f, D}(w)
\right| \geq \tau \right] &\leq 2 \exp\left( -\frac{ s \tau^2}{8W^2G^2}\right)
\label{eqn:concentration}
\end{align}
%%

%\eanote{Minor:  $w(x_i)$ doesn't seem explicitly defined.}

Finally, for linear functions $w$ ($x \mapsto w \cdot x$), let $\NZ(w) = \{ i
~|~ w_i \neq 0 \}$ denote the non-zero variables in $w$, so $\lznorm{w} =
|\NZ(w)|$. Then, we have the following Lemma. The proof appears in
Appendix~\ref{app:notation-class}.

\begin{lemma} \label{lemma:amsterdam} Let $D$ be a $\Delta$-smooth $G$-nice
distribution (Defn.~\ref{defn:afghanistan}), let $w \in \reals^n$ be a vector
and consider the corresponding linear function, $x \mapsto w \cdot x$. Then the
following are true:
\begin{enumerate}
\item For any $1 \leq i \leq n$, $w_i^2 \leq \frac{\ip{w}{w}}{\Delta^2}$.
\item There exists an $i$ such that $w_i^2 \leq
\frac{\ip{w}{w}}{|\NZ(w)|\Delta^2}$.
\end{enumerate}
\end{lemma}
