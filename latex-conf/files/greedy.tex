In this section, we present a different evolutionary mechanism for evolving
sparse linear functions. This algorithm essentially is an adaptation of a greedy
algorithm commonly known as orthogonal matching pursuit (OMP) in the signal
processing literature (see ~\cite{Donoho:2006-recovery, Tropp:2004-greed}). Our
analysis requires stronger properties on the distribution: we show that
$k$-sparse linear functions can be evolved with respect to $1/(2k)$-incoherent
$(\Delta, G)$-nice distributions (Defn.~\ref{defn:bhutan}). Here, the
selection rule used is selection using \emph{optimization}
($\optsel$).\footnote{Valiant showed that selection using optimization was
equivalent to selection using beneficial and neutral
mutations~\cite{Valiant:2009-evolvability}. However, this reduction uses
representation classes that may be somewhat complex. For restricted
representation classes, it is not clear that such a reduction holds. In
particular, the necessary ingredient seems to be \emph{polynomial-size}
memory.} Also, the algorithm is guaranteed to succeed only with
\emph{initialization} from the $0$ function.  Nevertheless, this evolutionary
algorithm is appealing due to its simplicity and because it never uses a
representation that is not a $k$-sparse linear function.

Recall that $f \in  \lin^k_{0, u}$ is the ideal (target)
function.\footnote{Here, we no longer need the fact that each coefficient in the
target linear function has magnitude at least $l$.} Let 
%
\[ R = \{w ~|~ \sparsity(w) \leq k, w_i \in [-B, B] \}, \]
%
where $B = 10uk/\Delta$. Now, starting from $w \in R$, define the action of the
mutator as follows (we will define the parameters $\lambda$ and $m$ later in the
proof of Theorem~\ref{thm:greedy}):
%%
\begin{enumerate}
%%
\item {\em Adding}: With probability $\lambda$, do the following. Recall that
$\NZ(w)$ denotes the non-zero entries of $w$. If $|\NZ(w)| < k$, choose $i \in [n]
\setminus \NZ(w)$ uniformly at random. Let $w^\prime$ be such that $w^\prime_j =
w_i$ for $j \neq i$, and choose $w^\prime_i \in [-B, B]$ uniformly at random. If
$\NZ(w)= k$, let $w^\prime = w$. Then, the multiset $\Neigh(w, \epsilon)$ is
populated by $m$ independent draws from the procedure just described.
%
\item With probability $1 - \lambda$, do the following:
%
\begin{enumerate}
\item {\em Identical}: With probability $1/2$, output $w^\prime = w$.
\item {\em Scaling}: With probability $1/4$, choose $\gamma \in [-1, 1]$ uniformly at
random and let $w^\prime = \gamma w$.
%
\item {\em Adjusting}: With probability $1/4$, do the following.  Pick $i \in \NZ(w)$
uniformly at random.  Let $w^\prime$ be such that $w^\prime_j = w_j$ for $j \neq
i$, and choose $w^\prime_i \in [-B, B]$ uniformly at random.
\end{enumerate}
Then, the multiset $\Neigh(w, \epsilon)$ is populated by $m$ independent draws
from the procedure just described.
\end{enumerate}

One thing to note in the above definition is that the mutations produced by the
mutator at any given time are correlated, \ie they are all either of the kind
that add a new variable, or all of the kind that just manipulate existing
variables.  At a high level, we prove the success of this mechanism as follows:
\begin{enumerate}
%
\item Using mutations of type ``scaling'' or ``adjusting,'' a representation
that is close to the \emph{best} in the space, \ie $f^S$, is evolved.
%
\item When the representation is (close to) the best possible using current
variables, adding one of the variables that is present in the \emph{ideal
function}, but not in the current representation, results in the greatest
reduction of expected loss. Thus, selection based on optimization would always
add a variable in $\NZ(f)$. By tuning $\lambda$ appropriately, it is ensured
that with high probability, candidate mutations that add new variables are not
chosen until evolution has had time to approach the \emph{best} representation
using existing variables.
%
\end{enumerate}

To complete the proof we establish the following claims.

\begin{claim} \label{claim:date} If $\ltwonorm{f^S - w} \leq
\sqrt{\epsilon}/{2k}$, then if $S \subsetneq \NZ(f)$, there exists $i \in \NZ(f)
\setminus S$ and $-B < a < b < B$, such that for any $\gamma \in [a, b]$,
$\loss_{f, D}(w + \gamma e^i) \leq \loss_{f, D}(w) - \epsilon/(4k^2)$ and for
any $j \not\in \NZ(f)$, $\beta \in [-B, B]$, $\loss_{f, D}(w + \beta e^j) \geq
\loss_{f, D}(w + \gamma e^i) + \epsilon/(4k^3)$. Furthermore, $b - a \geq
\sqrt{(k+1) \epsilon}/k^2$. \end{claim}

\begin{claim} \label{claim:elderberry} Conditioned on the mutator not outputting
mutations that add a new variable, with probability at least $\min\{1/16,
\ltwonorm{f^S - w}/(16k^2B)\}$, there exists a mutation that reduces the squared
loss by at least $\ltwonorm{f^S - w}^2/(12k^2)$. \end{claim}

The proofs of Claims~\ref{claim:date} and \ref{claim:elderberry} are not
difficult and are provided in Appendix~\ref{app:greedy}. Based on the above
claims we can prove the following theorem:

\begin{theorem} \label{thm:greedy} Let $\Dists$ be the class of
$1/(2k)$-incoherent $(\Delta, G)$-nice distributions over $\reals^n$
(Defn.~\ref{defn:bhutan}).
Then, the class $\lin^k_{0, u}$ is evolvable with respect to $\Dists$ by an
evolutionary algorithm, using the mutation algorithm described in this section,
selection rule $\optsel$, and the representation class $R = \lin^k_{0, B}$,
where $B = 10 uk/\Delta$.  Furthermore, the following are true:
%%
\begin{enumerate}
%
\item The number of generations $g$ is polynomial in $1/\epsilon$, $k$,
$1/\Delta$, but independent of the dimension~$n$.
%
\item The size function $s$, the number of points used to calculate the
empirical losses, depends polylogarithmically on $n$ and polynomially on the
remaining parameters.
\end{enumerate}
%%
\end{theorem}
\begin{proof} The proof is straightforward, although a bit heavy on notation; we
provide a sketch here. The mutator is as described in this section. Let
%%
\[ p = \min \left\{ \frac{1}{16}, \frac{\sqrt{\epsilon}}{64 k^3 B},
\frac{\sqrt{(k+1)\epsilon}}{k^2} \right\}, \]
%%
and let
%%
\[ \alpha = \min \left\{ \frac{\epsilon}{4k^3}, \frac{\epsilon}{192k^4} \right\}
= \frac{\epsilon}{192k^4}. \]
%%
Also, let $\tau = \alpha/5$ and let $t = 3\alpha/5$ be the \emph{tolerance
function}.

First, we show that between the ``rare'' time steps when the mutator outputs
mutations that add a new variable, evolution has enough time to \emph{stabilize}
(reach close to local optimal) using existing variables. To see this, consider a
sequence of coin tosses, where the probability of heads is $\lambda$ and the
probability of tails is $1 - \lambda$. Let $Y_i$ be the number of tails between
the $(i-1)\th$ and $i\th$ heads. Except with probability $\epsilon/(4(k+1))$,
$Y_i > \epsilon/(4(k+1)\lambda)$ by a simple union bound.  Also, by Markov's
inequality, except with probability $\epsilon/(4(k+1))$, $Y_i <
4(k+1)/(\epsilon\lambda)$. Thus, except with probability $\epsilon/2$, we have
$\epsilon/(4(k+1)\lambda) \leq Y_i \leq 4(k+1)/(\epsilon\lambda)$ for $i = 1, 2,
\ldots, k+1$. Let $g = 4(k+1)^2/(\epsilon\lambda) + (k+1)$. This ensures that,
except with probability $\epsilon/2$, after $g$ time steps, at least $(k+1)$
time steps where the mutator outputs mutations of type ``adding'' have occurred,
and the first $k$ of these occurrences are all separated by at least
$\epsilon/(4(k+1) \lambda)$ time steps of other types of mutations.

Also, let $m = p^{-1}\ln(4g/\epsilon)$ and let the \emph{size function} be $s =
(200gkG^2B^2/\alpha^2) \ln(4m/\epsilon)$.  These values ensure that for $g$
generations, except with probability $\epsilon/2$, the mutator always produces a
mutation that had probability at least $p$ of being produced (conditioned on the
type of mutations output by the mutator at that time step), and that for all the
representations concerned, $|\hat{\loss}_{f, D}(w) - \loss_{f, D}(w)| \leq
\tau$, where $\tau = \alpha/5$. Thus, allowing the process to fail with
probability $\epsilon$, we assume that none of the undesirable events have
occurred.

We will show that the steps with mutations other than ``adding'' are sufficient
to ensure that evolution reaches the (almost) best possible target with the
variables available to it. In particular, if the set of available variables is
$S$, the representation $w$ reached by evolution will satisfy $\ltwonorm{f^S -
w}^2 \leq \epsilon/(2k^2)$. For now, suppose that this is the case. 

We claim by induction that evolution never adds a ``wrong'' variable, \ie one
that is not present in the target function $f$. The base case is trivially true,
since the starting representation is $0$. Now suppose, just before a ``heads''
step, the representation is $w$, such that $S = \NZ(w)$ and $\ltwonorm{f^S - w}
\leq \epsilon/(2k^2)$. The current step is assumed to be a ``heads'' step, thus
the mutator has produced mutations by adding a new variable. Then, using
Claim~\ref{claim:date}, we know that there is a mutation $w^\prime$ in
$\Neigh(w, \epsilon)$ such that $\loss_{f, D}(w^\prime) < \loss_{f, D}(w) -
\epsilon/(4k^2)$ (obtained by adding a correct variable). Since $\alpha <
\epsilon/(4k^2)$ and $\tau = \alpha/5$, it must be the case that
$\hat{\loss}_{f, D}(w^\prime) \leq \hat{\loss}_{f, D}(w) - 3 \alpha/5$. This
ensures that the set $\best$, for selection rule $\optsel$ is not empty.
Furthermore, we claim that no mutation that adds an \emph{irrelevant} variable
can be in $\best$. Suppose $w^{\prime\prime}$ is a mutation that adds an
\emph{irrelevant} variable; according to Claim~\ref{claim:elderberry},
$\loss_{f, D}(w^{\prime\prime})> \loss_{f, D}(w^\prime) + \alpha$, and hence
$\hat{\loss}_{f, D}(w^{\prime\prime}) > \hat{\loss}_{f, D}(w^\prime) + t$. This ensures
that every representation in $\best$ corresponds to a mutation that adds some
\emph{relevant} variable. Thus, the evolutionary algorithm never adds any
\emph{irrelevant} variable.

Finally, note that during a ``tails'' step (when the mutator produces mutations of types
other than ``adding''), as long as $\ltwonorm{f^S - w}^2 \geq \epsilon/(4k^2)$,
there exists a mutation that reduces the expected loss by at least
$\epsilon^2/(192k^4) = \alpha$. This implies that the set $\best$ is non-empty
and for the values of tolerance $t = 3\alpha/5$ and $\tau = \alpha/5$, any
mutation from the set $\best$ reduces the expected loss by at least $\alpha/5$.
(This argument is identical to the one in Theorem~\ref{thm:sparse_linear}.)
Since the maximum loss is at most $4kB^2G^2$ for the class of distributions and
a representation $w$ from the set $R$; in at most $20kB^2G^2/\alpha$ steps, a
representation satisfying $\loss_{f, D}(w) \leq \epsilon/(4k^2)$ must be
reached. Note that once such a representation is reached, it is ensured that the
loss does not increase substantially, since with probability at least $1/2$, the
mutator outputs the same representation. Hence, it is guaranteed that there is
always a neutral mutation. Thus, before the next ``heads'' step, it must be the
case that $\ltwonorm{f^S - w}^2 \leq \epsilon/2k^2$. If $\lambda$ is set to
$\epsilon\alpha/(80 k(k+1)B^2G^2)$, the evolutionary algorithm using the
selection rule $\optsel$ succeeds. 

It is readily verified that the values of $g$ and $s$ satisfy the claims in the
statement of the theorem.
\end{proof}

%% VK %% In the ensuing discussion, we will prove that for any target function
%% VK %% $w^* \in C^k_{l,u}$, if $w \in R$ is such that
%% VK %% $\lerr_D(w, w^*) = \ltwonorm{w - w^*}^2 > \epsilon$,
%% VK %% then the mutator outputs a mutation which decreases the error by a
%% VK %% non-negligible amount with non-negligible probability.
%% VK %% We will establish our claim by proving the following claims.
%% VK %% 
%% VK %% \begin{enumerate}
%% VK %% \item[Claim A] If $\ltwonorm{w} \ge 2 \ltwonorm{w^*_S}$, then for $M \ge 1$,
%% VK %% with probability at least $1/16 - \lambda/8 \ge $ \eanote{something} the mutator outputs
%% VK %% a mutation that reduces $\lerror$ by at least $\ltwonorm{w - w^*_S}^2 / 12$.
%% VK %% \item[Claim B] If $\ltwonorm{w} \le 2 \ltwonorm{w^*_S}$ and
%% VK %% $\ltwonorm{w - w^*_S} \ge \alpha$, then with probability at least $\alpha$,
%% VK %% there exists a mutation that decreases the $\lerror$ by at least $\alpha$.
%% VK %% \item[Claim C] If $\Delta^2 > 1 - 1/(2 K - 1)$ and
%% VK %% $\ltwonorm{w - w^*_S}^2 < \epsilon$, then $w^\prime$ will be
%% VK %% such that either $\sparseset(w^\prime) = \sparseset(w)$ and
%% VK %% $\ltwonorm{w^\prime - w^*_S}^2 < \epsilon$, or
%% VK %% $\sparseset(w^\prime) \setminus \sparseset(w) = \{i\}$ where $i \in \sparseset(w^*)$
%% VK %% and $w^\prime$ reduces $\lerror$ \eanote{by at least some amount}.
%% VK %% \end{enumerate}
%% VK %% 
%% VK %% To establish Claims A and B above, we follow the lines of reasoning from the
%% VK %% previous section.
%% VK %% For Claim A, we consider the case where selection has chosen the scaling class
%% VK %% of mutants.  With the assumption that $\ltwonorm{w} \ge 2 \ltwonorm{w_S^2}$ and
%% VK %% for each of the generated mutants $\gamma w$ with $\gamma \in [1/2, 3/4]$,
%% VK %% we obtain
%% VK %% 
%% VK %% \[
%% VK %% \ltwonorm{\gamma w - w^*_S}^2 \le \ltwonorm{w - w^*_S}^2 - \frac{1}{12}\ltwonorm{w - w^*_S}^2.
%% VK %% \]
%% VK %% 
%% VK %% For Claim B, \dots
%% VK %% 
%% VK %% For Claim C, in the case that selection chose the adding class of mutations,
%% VK %% we must show that the new index $i \in \sparseset(w^*)$, where
%% VK %% $\{i\} = \sparseset(w^\prime) \setminus \sparseset(w)$.
%% VK %% We show that among the set of mutants output by the mutator, with high probability
%% VK %% there will exist at least one mutant such that it reduces $\lerror$ by something.
%% VK %% \eanote{Explain how this relates to the proof in Donoho and give the correct citation(s)}.
%% VK %% 
%% VK %% Let $\hat{w}$ be the best approximation of $w^*$ such that $\hat{w}_i \in [-B, B]$
%% VK %% for some $i \in [n] \setminus \sparseset(w)$ and $\hat{w}_j = w_j$ for $j \neq i$;
%% VK %% in other words, $\hat{w}$ is the best possible output achieved by mutating $w$ with
%% VK %% one adding step.
%% VK %% In expectation, if $\lznorm{w} = k$, then $M / (n - k)$ of the generated mutants
%% VK %% $v$ will have $\sparseset(v) = \sparseset(\hat{w})$, and each will satisfy
%% VK %% $|v_i - \hat{w}_i| \le \delta$, $\delta < B$, and so
%% VK %% $\ltwonorm{v - \hat{w}} \le \delta$ with probability $\delta/2B$.
%% VK %% Thus in expectation, for $\delta > 2B(n-k) / M$, at least one mutant will be
%% VK %% ``$\delta$-close'' to $\hat{w}$.
%% VK %% Below we argue that the remaining mutants with
%% VK %% $\sparseset(v) \neq \sparseset(\hat{w})$ will not be better than this mutant by
%% VK %% a margin of at least \eanote{fill in this condition}.
%% VK %% 
%% VK %% We must show that $i \in \sparseset(w^*)$ and furthermore that
%% VK %% \eanote{fill in this condition}.
%% VK %% Selection starts with the initial representation, $w=0$.
%% VK %% Without loss of generality, arrange $w^*$ so that $\sparseset(w^*) = [K]$,
%% VK %% in decreasing order of the values $\vert w^*_k \vert \sqrt{\var(X_k)}$.
%% VK %% Initially, the residual $w^* - w$ is $w^*$.
%% VK %% Let $w^*_{\{j\}}$ be the best approximation of $w^*$ with
%% VK %% $\sparseset(w^*_{\{j\}}) = \{j\}$, so $w^*_{\{j\}}$ \dots
%% VK %% With high probability, the mutator generates at least one mutant
%% VK %% $\hat{w}_{\{j\}}$ for each $j \in [n]$ that minimizes the residual to within
%% VK %% $\delta$ of the optimal solution restricted to $\{j\}$, i.e.,
%% VK %% $\ltwonorm{\hat{w}_{\{j\}} - w^*}^2 < \ltwonorm{w^*_{\{j\}} - w^*}^2 + \delta$.
%% VK %% The $\lerror$ of $w^*_{\{j\}}$ is
%% VK %% \eanote{Check the last step below!}
%% VK %% 
%% VK %% \begin{align*}
%% VK %% \ltwonorm{w^*_{\{j\}} - w^*}^2 = \min_{w_{\{j\}}}{\ltwonorm{w_{\{j\}} - w^*}}^2
%% VK %% &= \bigg \Vert e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2} - w^* \bigg \Vert^2 \\
%% VK %% &= \ltwonorm{w^*}^2 - 2 \bigg \langle e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2}, w^* \bigg \rangle + \bigg \Vert e_j \frac{\ip{e_j}{w^*}}{\ltwonorm{e_j}^2} \bigg \Vert^2 \\
%% VK %% &= \ltwonorm{w^*}^2 - \frac{(\ip{e_j}{w^*})^2}{\ltwonorm{e_j}^2} \ge 0.
%% VK %% \end{align*}
%% VK %% 
%% VK %% \noindent Note that
%% VK %% 
%% VK %% \[
%% VK %% \frac{(\ip{e_j}{w^*})^2}{\ltwonorm{e_j}^2}
%% VK %% = \frac{(\E[(e_j \cdot X)(w^* \cdot X)])^2}{\E[(e_j \cdot X)(e_j \cdot X)]}
%% VK %% = \frac{(\E[X_j (w^* \cdot X)])^2}{\E[X_j^2]}
%% VK %% = \frac{(\sum_{k=1}^K w^*_k \E[X_j X_k]))^2}{\var(X_j)}
%% VK %% \]
%% VK %% 
%% VK %% For selection to choose a mutant $\hat{w}_{\{j\}}$ with
%% VK %% $j \in \sparseset(w^*) = [K]$, it must be the case that for any $i > K$,
%% VK %% 
%% VK %% \[
%% VK %% \bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
%% VK %% > \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert + \eanote{~something}.
%% VK %% \]
%% VK %% 
%% VK %% We construct lower and upper bounds for the left- and right-hand sides,
%% VK %% respectively, making use of the fact that
%% VK %% $|\corr(X_i, X_j)| \leq 1 - \Delta^2$ from Lemma~\ref{} and also
%% VK %% that we arranged the $\vert w^*_k \vert \sqrt{\var(X_k)}$ in decreasing order.
%% VK %% On the left,
%% VK %% 
%% VK %% \begin{align*}
%% VK %% \bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
%% VK %% &= \frac{\vert\sum_{k=1}^K w^*_k \E[X_1 X_k]\vert}{\sqrt{\var(X_1)}} \\
%% VK %% &\ge \frac{\vert w^*_1 \E[X_1 X_1]\vert}{\sqrt{\var(X_1)}} - \frac{\sum_{k=2}^K \vert w^*_k \E[X_1 X_k] \vert}{\sqrt{\var(X_1)}} \\
%% VK %% &= \vert w^*_1 \vert \sqrt{\var(X_1)} - \sum_{k=2}^K \vert w^*_k \vert \sqrt{\var(X_k)}~\vert \corr(X_1, X_k) \vert \\
%% VK %% &\ge \vert w^*_1 \vert \sqrt{\var(X_1)}(1 - (1 - \Delta^2) (K-1))
%% VK %% \end{align*}
%% VK %% 
%% VK %% \noindent On the right,
%% VK %% 
%% VK %% \begin{align*}
%% VK %% \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert
%% VK %% = \frac{\vert\sum_{k=1}^K w^*_k \E[X_i X_k]\vert}{\sqrt{\var(X_i)}}
%% VK %% \le \sum_{k=1}^K \vert w^*_k \vert \frac{\vert \E[X_i X_k]\vert}{\sqrt{\var(X_i)}}
%% VK %% &= \sum_{k=1}^K \vert w^*_k \vert \sqrt{\var(X_k)} \corr(X_i, X_k) \\
%% VK %% &\le \vert w^*_1 \vert \sqrt{\var(X_1)} (1 - \Delta^2) K.
%% VK %% \end{align*}
%% VK %% 
%% VK %% \noindent Putting these bounds together now gives the requirement
%% VK %% 
%% VK %% \[
%% VK %% \bigg\vert \frac{\ip{e_1}{w^*}}{\ltwonorm{e_1}} \bigg\vert
%% VK %% \ge \vert w^*_1 \vert \sqrt{\var(X_1)}(1 - (1 - \Delta^2) (K-1))
%% VK %% > \vert w^*_1 \vert \sqrt{\var(X_1)} (1 - \Delta^2) K
%% VK %% \ge \bigg\vert \frac{\ip{e_i}{w^*}}{\ltwonorm{e_i}} \bigg\vert,
%% VK %% \]
%% VK %% 
%% VK %% \noindent and so
%% VK %% 
%% VK %% \begin{align*}
%% VK %% 1 - (1 - \Delta^2) (K-1) = 2 - K + \Delta^2 K - \Delta^2 &> (1 - \Delta^2) K \\
%% VK %% %\Rightarrow & \qquad 1 + (1 - \Delta^2) > 2 (1 - \Delta^2) K
%% VK %% %\qquad \Rightarrow \qquad K < \frac{1}{2} \biggl( 1 + \frac{1}{1 - \Delta^2} \biggr)
%% VK %% \Rightarrow \qquad \Delta^2 &> 1 - \frac{1}{2 K - 1}
%% VK %% \end{align*}
%% VK %% 
%% VK %% \noindent  This is the condition for $\Delta^2$ in Claim C.
%% VK %% \eanote{Finally, argue that subsequent steps work too.}
%% VK %% \eanote{And then we should say something about the CS analog.}
